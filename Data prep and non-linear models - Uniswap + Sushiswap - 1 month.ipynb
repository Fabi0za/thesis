{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "#To import parquet file with gas prices\n",
    "import pyarrow\n",
    "\n",
    "#For plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as offline\n",
    "import plotly.subplots as sp\n",
    "import plotly.io as pio\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "#For feature preprocessing and for tree-based models\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Bayesian optimization (one for torch one for sklearn)\n",
    "from skopt import BayesSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "#For neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torch_optimizer as optim1\n",
    "import torch.optim as optim\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5342b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe with pool contract, pair and volume_usd until a certain timepoint\n",
    "Uniswap_pools_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/pool_Uniswap.csv\")\n",
    "#Dataframe with USD and swap prices\n",
    "Uniswap_prices_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/prices_Uniswap.csv\")\n",
    "#Dataframe with swap volume\n",
    "Uniswap_volume_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/volume_Uniswap.csv\")\n",
    "#Dataframe with gas fees\n",
    "gas_fees = pd.read_parquet('/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/gas_ts_block.par', engine='pyarrow')\n",
    "#Dataframe with liquidity pool tokens\n",
    "Uniswap_pool_tokens_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/liquidity_tokens_uniswap.csv\")\n",
    "\n",
    "#Same for Sushiswap\n",
    "Sushiswap_pools_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/pool_Sushiswap.csv\")\n",
    "Sushiswap_prices_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/prices_Sushiswap.csv\")\n",
    "Sushiswap_volume_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/volume_Sushiswap.csv\")\n",
    "Sushiswap_pool_tokens_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/liquidity_tokens_sushiswap.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding a few Sushiswap pools that don't have enough data (FARM / WETH, KP3R / WETH, USDC / USDT)\n",
    "exclude_pools = [\"0x69b39b89f9274a16e8a19b78e5eb47a4d91dac9e\", \"0xaf988aff99d3d0cb870812c325c588d8d8cb7de8\", \"0xd86a120a06255df8d4e2248ab04d4267e23adfaa\"] \n",
    "\n",
    "Sushiswap_pools_df = Sushiswap_pools_df[~Sushiswap_pools_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_prices_df = Sushiswap_prices_df[~Sushiswap_prices_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_volume_df = Sushiswap_volume_df[~Sushiswap_volume_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df[~Sushiswap_pool_tokens_df[\"pool_address\"].isin(exclude_pools)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fa830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting date columns to datetime\n",
    "Uniswap_pools_df['date'] = pd.to_datetime(Uniswap_pools_df['date'])\n",
    "Uniswap_prices_df['date'] = pd.to_datetime(Uniswap_prices_df['date'])\n",
    "Uniswap_volume_df['date'] = pd.to_datetime(Uniswap_volume_df['date'])   \n",
    "Uniswap_pool_tokens_df['date'] = pd.to_datetime(Uniswap_pool_tokens_df['date'])\n",
    "Sushiswap_pools_df['date'] = pd.to_datetime(Sushiswap_pools_df['date'])\n",
    "Sushiswap_prices_df['date'] = pd.to_datetime(Sushiswap_prices_df['date'])\n",
    "Sushiswap_volume_df['date'] = pd.to_datetime(Sushiswap_volume_df['date'])   \n",
    "Sushiswap_pool_tokens_df['date'] = pd.to_datetime(Sushiswap_pool_tokens_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Uniswap_pool_tokens_df and Sushiswap_pool_tokens_df\n",
    "#Remove columns\n",
    "Uniswap_pool_tokens_df = Uniswap_pool_tokens_df.drop(['Unnamed: 0'], axis=1)\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df.drop(['Unnamed: 0'], axis=1)\n",
    "#Rename columns\n",
    "Uniswap_pool_tokens_df = Uniswap_pool_tokens_df.rename(columns={\"totalSupply\": \"pool_token_amount\", 'pool_address': 'pool'})\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df.rename(columns={\"totalSupply\": \"pool_token_amount\", 'pool_address': 'pool'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba77337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Sushiswap_volume_df: Rename columns\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.rename(columns={\"token_it\": \"token_in\", '_col3': 'max_token_pair'})\n",
    "\n",
    "#Only take last observation for a day (df currently contains hourly observations)\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.sort_values('hour').groupby(['pool', 'date'], as_index=False).last()\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.drop('hour', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d627765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating dfs to get one df\n",
    "complete_volume_df = pd.concat([Uniswap_volume_df, Sushiswap_volume_df])\n",
    "complete_pool_tokens_df = pd.concat([Uniswap_pool_tokens_df, Sushiswap_pool_tokens_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e12e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average daily gas fees\n",
    "gas_fees['ts'] = pd.to_datetime(gas_fees['ts'])\n",
    "gas_fees = gas_fees.set_index('ts')\n",
    "average_daily_gas_fees = gas_fees.resample('D')['avg_gas'].mean()\n",
    "average_daily_gas_fees = average_daily_gas_fees.reset_index()\n",
    "average_daily_gas_fees = average_daily_gas_fees.rename(columns={\"ts\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1905d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a mapping that allows correlating smart contract addresses to liq. pairs\n",
    "pool_mapping_Uniswap = {\n",
    "    '0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc': ('USDC', 'WETH'),\n",
    "    '0x0d4a11d5eeaac28ec3f61d100daf4d40471f1852': ('WETH', 'USDT'),\n",
    "    '0xa478c2975ab1ea89e8196811f51a7b7ade33eb11': ('DAI', 'WETH'),\n",
    "    '0xbb2b8038a1640196fbe3e38816f3e67cba72d940': ('WBTC', 'WETH'),\n",
    "    '0x2fdbadf3c4d5a8666bc06645b8358ab803996e28': ('YFI', 'WETH'),\n",
    "    '0xd3d2e2692501a5c9ca623199d38826e513033a17': ('UNI', 'WETH'),\n",
    "    '0xa2107fa5b38d9bbd2c461d6edf11b11a50f6b974': ('LINK', 'WETH'),\n",
    "    '0xc5be99a02c6857f9eac67bbce58df5572498f40c': ('WETH', 'AMPL'),\n",
    "    '0x3041cbd36888becc7bbcbc0045e3b1f144466f5f': ('USDC', 'USDT'),\n",
    "    '0xdfc14d2af169b0d36c4eff567ada9b2e0cae044f': ('AAVE', 'WETH'),\n",
    "    '0xab3f9bf1d81ddb224a2014e98b238638824bcf20': ('LEND', 'WETH'),\n",
    "    '0xc2adda861f89bbb333c90c492cb837741916a225': ('MKR', 'WETH'),\n",
    "    '0x43ae24960e5534731fc831386c07755a2dc33d47': ('SNX', 'WETH'),\n",
    "    '0x3da1313ae46132a397d90d95b1424a9a7e3e0fce': ('WETH', 'CRV'),\n",
    "    '0x87febfb3ac5791034fd5ef1a615e9d9627c2665d': ('KP3R', 'WETH'),\n",
    "    '0x32ce7e48debdccbfe0cd037cc89526e4382cb81b': ('CORE', 'WETH'),\n",
    "    '0xcffdded873554f362ac02f8fb1f02e5ada10516f': ('COMP', 'WETH'),\n",
    "    '0xdc98556ce24f007a5ef6dc1ce96322d65832a819': ('PICKLE', 'WETH'),\n",
    "    '0x56feaccb7f750b997b36a68625c7c596f0b41a58': ('FARM', 'WETH'),\n",
    "    '0xae461ca67b15dc8dc81ce7615e0320da1a9ab8d5': ('DAI', 'USDC'),\n",
    "    '0xddf9b7a31b32ebaf5c064c80900046c9e5b7c65f': ('CREAM', 'WETH'),\n",
    "    '0xb20bd5d04be54f870d5c0d3ca85d82b34b836405': ('DAI', 'USDT'),\n",
    "    '0x88d97d199b9ed37c29d846d00d443de980832a22': ('UMA', 'WETH'),\n",
    "}\n",
    "\n",
    "pool_mapping_Sushiswap = {\n",
    "    '0x397ff1542f962076d0bfe58ea045ffa2d347aca0': ('USDC', 'WETH'),\n",
    "    '0x06da0fd433c1a5d7a4faa01111c044910a184553': ('WETH', 'USDT'),\n",
    "    '0xc3d03e4f041fd4cd388c549ee2a29a9e5075882f': ('DAI', 'WETH'),\n",
    "    '0xceff51756c56ceffca006cd410b03ffc46dd3a58': ('WBTC', 'WETH'),\n",
    "    '0x088ee5007c98a9677165d78dd2109ae4a3d04d0c': ('YFI', 'WETH'),\n",
    "    '0xdafd66636e2561b0284edde37e42d192f2844d40': ('UNI', 'WETH'),\n",
    "    '0xc40d16476380e4037e6b1a2594caf6a6cc8da967': ('LINK', 'WETH'),\n",
    "    '0xcb2286d9471cc185281c4f763d34a962ed212962': ('WETH', 'AMPL'),\n",
    "    '0xd86a120a06255df8d4e2248ab04d4267e23adfaa': ('USDC', 'USDT'),\n",
    "    '0xd75ea151a61d06868e31f8988d28dfe5e9df57b4': ('AAVE', 'WETH'),\n",
    "    '0xba13afecda9beb75de5c56bbaf696b880a5a50dd': ('MKR', 'WETH'),\n",
    "    '0xa1d7b2d891e3a1f9ef4bbc5be20630c2feb1c470': ('SNX', 'WETH'),\n",
    "    '0x58dc5a51fe44589beb22e8ce67720b5bc5378009': ('WETH', 'CRV'),\n",
    "    '0xaf988aff99d3d0cb870812c325c588d8d8cb7de8': ('KP3R', 'WETH'),\n",
    "    '0x68c6d02d44e16f1c20088731ab032f849100d70f': ('CORE', 'WETH'),\n",
    "    '0x31503dcb60119a812fee820bb7042752019f2355': ('COMP', 'WETH'),\n",
    "    '0x269db91fc3c7fcc275c2e6f22e5552504512811c': ('PICKLE', 'WETH'),\n",
    "    '0xf169cea51eb51774cf107c88309717dda20be167': ('CREAM', 'WETH'),\n",
    "    '0x001b6450083e531a5a7bf310bd2c1af4247e23d4': ('UMA', 'WETH'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e97d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matching pools to smart contract addresses\n",
    "Uniswap_pools_df['symbol_1'] = Uniswap_pools_df['pool'].map(lambda x: pool_mapping_Uniswap.get(x, (None, None))[0])\n",
    "Uniswap_pools_df['symbol_2'] = Uniswap_pools_df['pool'].map(lambda x: pool_mapping_Uniswap.get(x, (None, None))[1])\n",
    "Sushiswap_pools_df['symbol_1'] = Sushiswap_pools_df['pool'].map(lambda x: pool_mapping_Sushiswap.get(x, (None, None))[0])\n",
    "Sushiswap_pools_df['symbol_2'] = Sushiswap_pools_df['pool'].map(lambda x: pool_mapping_Sushiswap.get(x, (None, None))[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting the token amounts for Uniswap pools\n",
    "pool_list_1 = [\"0x2fdbadf3c4d5a8666bc06645b8358ab803996e28\", \"0x32ce7e48debdccbfe0cd037cc89526e4382cb81b\", \"0x3da1313ae46132a397d90d95b1424a9a7e3e0fce\", \"0x43ae24960e5534731fc831386c07755a2dc33d47\", \"0x56feaccb7f750b997b36a68625c7c596f0b41a58\", \"0x87febfb3ac5791034fd5ef1a615e9d9627c2665d\", \"0x88d97d199b9ed37c29d846d00d443de980832a22\", \"0xa2107fa5b38d9bbd2c461d6edf11b11a50f6b974\", \"0xa478c2975ab1ea89e8196811f51a7b7ade33eb11\", \"0xab3f9bf1d81ddb224a2014e98b238638824bcf20\", \"0xc2adda861f89bbb333c90c492cb837741916a225\", \"0xcffdded873554f362ac02f8fb1f02e5ada10516f\", \"0xd3d2e2692501a5c9ca623199d38826e513033a17\", \"0xdc98556ce24f007a5ef6dc1ce96322d65832a819\", \"0xddf9b7a31b32ebaf5c064c80900046c9e5b7c65f\", \"0xdfc14d2af169b0d36c4eff567ada9b2e0cae044f\"]\n",
    "\n",
    "for pool in pool_list_1:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "\n",
    "pool_list_2 = [\"0x3041cbd36888becc7bbcbc0045e3b1f144466f5f\"]    \n",
    "    \n",
    "for pool in pool_list_2:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_3 = [\"0xae461ca67b15dc8dc81ce7615e0320da1a9ab8d5\", \"0xb20bd5d04be54f870d5c0d3ca85d82b34b836405\", \"0x0d4a11d5eeaac28ec3f61d100daf4d40471f1852\"]    \n",
    "    \n",
    "for pool in pool_list_3:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_4 = [\"0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc\"]    \n",
    "    \n",
    "for pool in pool_list_4:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_5 = [\"0xbb2b8038a1640196fbe3e38816f3e67cba72d940\"]    \n",
    "    \n",
    "for pool in pool_list_5:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e8\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_6 = [\"0xc5be99a02c6857f9eac67bbce58df5572498f40c\"]    \n",
    "    \n",
    "for pool in pool_list_6:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting the token amounts for Sushiswap pools\n",
    "pool_list_1 = [\"0x088ee5007c98a9677165d78dd2109ae4a3d04d0c\", \"0x68c6d02d44e16f1c20088731ab032f849100d70f\", \"0x58dc5a51fe44589beb22e8ce67720b5bc5378009\", \"0xa1d7b2d891e3a1f9ef4bbc5be20630c2feb1c470\", \"0xaf988aff99d3d0cb870812c325c588d8d8cb7de8\", \"0x001b6450083e531a5a7bf310bd2c1af4247e23d4\", \"0xc40d16476380e4037e6b1a2594caf6a6cc8da967\", \"0xc3d03e4f041fd4cd388c549ee2a29a9e5075882f\", \"0xba13afecda9beb75de5c56bbaf696b880a5a50dd\", \"0x31503dcb60119a812fee820bb7042752019f2355\", \"0xdafd66636e2561b0284edde37e42d192f2844d40\", \"0x269db91fc3c7fcc275c2e6f22e5552504512811c\", \"0xf169cea51eb51774cf107c88309717dda20be167\", \"0xd75ea151a61d06868e31f8988d28dfe5e9df57b4\"]\n",
    "\n",
    "for pool in pool_list_1:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "\n",
    "pool_list_2 = [\"0xd86a120a06255df8d4e2248ab04d4267e23adfaa\"]    \n",
    "    \n",
    "for pool in pool_list_2:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_3 = [\"0x06da0fd433c1a5d7a4faa01111c044910a184553\"]    \n",
    "    \n",
    "for pool in pool_list_3:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_4 = [\"0x397ff1542f962076d0bfe58ea045ffa2d347aca0\"]    \n",
    "    \n",
    "for pool in pool_list_4:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_5 = [\"0xceff51756c56ceffca006cd410b03ffc46dd3a58\"]    \n",
    "    \n",
    "for pool in pool_list_5:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e8\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_6 = [\"0xcb2286d9471cc185281c4f763d34a962ed212962\"]    \n",
    "    \n",
    "for pool in pool_list_6:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef613156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a flag indicating which DEX the data is from and concatenating the two dfs\n",
    "Uniswap_pools_df['Uniswap'] = '1'\n",
    "Sushiswap_pools_df['Sushiswap'] = '1'\n",
    "complete_pools_df = pd.concat([Uniswap_pools_df, Sushiswap_pools_df])\n",
    "complete_pools_df = complete_pools_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32327682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating swap rates between tokens based on amount of tokens in pool\n",
    "complete_pools_df[\"avg_swap_price\"] = complete_pools_df[\"y\"] / complete_pools_df[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating USD swap rates and rearranging direction of token swap rates\n",
    "Sushiswap_prices_df = Sushiswap_prices_df.rename(columns={\"median_usd_price_token_sold\": \"avg_usd_price_token_sold\", \"median_usd_price_token_bought\": \"avg_usd_price_token_bought\", \"median_swap_price\": \"avg_swap_price\"})\n",
    "\n",
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "def filter_and_append(df, filtered_df, pool_mapping):\n",
    "    for pool_address, (token_a, token_b) in pool_mapping.items():\n",
    "        filtered_rows = df[(df['pool'] == pool_address) &\n",
    "                           ((df['token_bought_symbol'] == token_a) & (df['token_sold_symbol'] == token_b) |\n",
    "                            (df['token_bought_symbol'] == token_b) & (df['token_sold_symbol'] == token_a))]\n",
    "\n",
    "        grouped_rows = filtered_rows.groupby('date')\n",
    "\n",
    "        for date, group in grouped_rows:\n",
    "            #Check if both directions are present (account for the fact that trades can happen in both directions)\n",
    "            if len(group) == 2:\n",
    "                #Keep the preferred direction (based on what was defined in dict above)\n",
    "                preferred_row = group[(group['token_bought_symbol'] == token_a) & (group['token_sold_symbol'] == token_b)]\n",
    "            else:\n",
    "                preferred_row = group\n",
    "\n",
    "                #Check if direction needs to be switched\n",
    "                if preferred_row.iloc[0]['token_bought_symbol'] != token_a:\n",
    "                    temp_token_bought_symbol = preferred_row.loc[:, 'token_bought_symbol'].copy()\n",
    "                    preferred_row.loc[:, 'token_bought_symbol'] = preferred_row.loc[:, 'token_sold_symbol']\n",
    "                    preferred_row.loc[:, 'token_sold_symbol'] = temp_token_bought_symbol\n",
    "\n",
    "                    #Switch avg_usd_price_token_sold and avg_usd_price_token_bought\n",
    "                    temp_avg_usd_price_token_sold = preferred_row.loc[:, 'avg_usd_price_token_sold'].copy()\n",
    "                    preferred_row.loc[:, 'avg_usd_price_token_sold'] = preferred_row.loc[:, 'avg_usd_price_token_bought']\n",
    "                    preferred_row.loc[:, 'avg_usd_price_token_bought'] = temp_avg_usd_price_token_sold\n",
    "\n",
    "                    #Calculate avg_swap_price in opposite direction\n",
    "                    preferred_row.loc[:, 'avg_swap_price'] = 1 / preferred_row.loc[:, 'avg_swap_price']\n",
    "            filtered_df = pd.concat([filtered_df, preferred_row], ignore_index=True)\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df = filter_and_append(Uniswap_prices_df, filtered_df, pool_mapping_Uniswap)\n",
    "filtered_df = filter_and_append(Sushiswap_prices_df, filtered_df, pool_mapping_Sushiswap)\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df[['avg_usd_price_token_sold', 'avg_usd_price_token_bought', 'avg_swap_price']] = filtered_df[['avg_usd_price_token_sold', 'avg_usd_price_token_bought', 'avg_swap_price']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace extreme outliers with the last non-outlier price: Using a rolling-window approach given the outlier nature of the data\n",
    "#Window size is number of day taken into account and threshold determines how many standard deviations a value has to deviate to be considered an outlier\n",
    "def remove_rolling_outliers(df, pool_name, window_size=30, threshold=1.5):\n",
    "    pool_df = df[df['pool'] == pool_name].sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    #Rolling mean and stdev calculation\n",
    "    pool_df['rolling_mean'] = pool_df['avg_swap_price'].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    pool_df['rolling_std'] = pool_df['avg_swap_price'].rolling(window=window_size, center=True, min_periods=1).std()\n",
    "\n",
    "    #Identification of outliers\n",
    "    outliers = (pool_df['avg_swap_price'] < (pool_df['rolling_mean'] - threshold * pool_df['rolling_std'])) | \\\n",
    "               (pool_df['avg_swap_price'] > (pool_df['rolling_mean'] + threshold * pool_df['rolling_std']))\n",
    "\n",
    "    #Iterating through the rows and updating values if current row is outlier\n",
    "    for index, row in pool_df.loc[outliers].iterrows():\n",
    "        outlier_date = row['date']\n",
    "\n",
    "        #Updating outlier value with last non-outlier value\n",
    "        last_valid_day = pool_df.loc[(pool_df['date'] < outlier_date) & ~outliers].sort_values('date', ascending=False).head(1)\n",
    "        if not last_valid_day.empty:\n",
    "            last_values = last_valid_day[['avg_swap_price', 'avg_usd_price_token_sold', 'avg_usd_price_token_bought']].values[0]\n",
    "            pool_df.loc[index, ['avg_swap_price', 'avg_usd_price_token_sold', 'avg_usd_price_token_bought']] = last_values\n",
    "\n",
    "    return pool_df.drop(columns=['rolling_mean', 'rolling_std'])\n",
    "\n",
    "#Removing the outliers\n",
    "pool_names = filtered_df['pool'].unique()\n",
    "processed_data = pd.concat([remove_rolling_outliers(filtered_df, pool) for pool in pool_names]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing beginning of month prices to check whether they look fine\n",
    "#Creating a new column to store DEX name\n",
    "processed_data[\"platform\"] = np.nan\n",
    "for pool in processed_data[\"pool\"].unique():\n",
    "    if pool in pool_mapping_Uniswap:\n",
    "        processed_data.loc[processed_data[\"pool\"] == pool, \"platform\"] = \"Uniswap\"\n",
    "    elif pool in pool_mapping_Sushiswap:\n",
    "        processed_data.loc[processed_data[\"pool\"] == pool, \"platform\"] = \"Sushiswap\"\n",
    "\n",
    "processed_data['date'] = pd.to_datetime(processed_data['date'])\n",
    "processed_data.set_index('date', inplace=True)\n",
    "\n",
    "#Separating Uniswap and Sushiswap data\n",
    "processed_data_Uniswap = processed_data[processed_data[\"platform\"] == \"Uniswap\"]\n",
    "processed_data_Sushiswap = processed_data[processed_data[\"platform\"] == \"Sushiswap\"]\n",
    "\n",
    "#Function to process data\n",
    "def process_data(processed_data):\n",
    "    grouped_df = processed_data.groupby(\"pool\")\n",
    "    bom_df = pd.DataFrame()\n",
    "\n",
    "    #Resampling the data to get beginning of month data\n",
    "    for pool, group_df in grouped_df:\n",
    "        \n",
    "        pool_bom_df = group_df.resample(\"MS\").last()\n",
    "        bom_df = pd.concat([bom_df, pool_bom_df], ignore_index=False)\n",
    "    \n",
    "    #Interpolating nan values\n",
    "    bom_df.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "\n",
    "    #Storing beginning of month prices for each unique token\n",
    "    unique_tokens = pd.concat([bom_df[\"token_bought_symbol\"], bom_df[\"token_sold_symbol\"]]).unique()\n",
    "    bom_usd_prices = pd.DataFrame(index=bom_df.index.unique(), columns=unique_tokens)\n",
    "    for token in unique_tokens:\n",
    "        token_rows = bom_df[(bom_df[\"token_bought_symbol\"] == token) | (bom_df[\"token_sold_symbol\"] == token)]\n",
    "        token_rows = token_rows.loc[~token_rows.index.duplicated(keep='first')]\n",
    "        bom_usd_prices[token] = token_rows.apply(lambda row: row[\"avg_usd_price_token_bought\"] if row[\"token_bought_symbol\"] == token else row[\"avg_usd_price_token_sold\"], axis=1)\n",
    "    \n",
    "    #Interpolating usd prices that are still missing\n",
    "    bom_usd_prices.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "    return bom_usd_prices\n",
    "\n",
    "#Apply function to Uniswap and Sushiswap data separately\n",
    "bom_usd_prices_Uniswap = process_data(processed_data_Uniswap)\n",
    "bom_usd_prices_Sushiswap = process_data(processed_data_Sushiswap)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "bom_usd_prices_Uniswap.plot(ax=ax[0])\n",
    "ax[0].set_ylabel(\"USD Price\")\n",
    "ax[0].set_title(\"USD Price at the Beginning of Each Month for Unique Tokens on Uniswap\")\n",
    "ax[0].legend(title=\"Tokens\")\n",
    "bom_usd_prices_Sushiswap.plot(ax=ax[1])\n",
    "ax[1].set_ylabel(\"USD Price\")\n",
    "ax[1].set_xlabel(\"Date\")\n",
    "ax[1].set_title(\"USD Price at the Beginning of Each Month for Unique Tokens on Sushiswap\")\n",
    "ax[1].legend(title=\"Tokens\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the dataframes\n",
    "complete_pools_df = pd.merge(complete_pools_df, complete_volume_df, on=['pool', 'date'])\n",
    "\n",
    "#Removing unnecessary columns\n",
    "complete_pools_df = complete_pools_df.drop(columns = [\"max_token_pair\", \"token_in\", \"token_out\"])\n",
    "\n",
    "#Calculating order flow imbalance\n",
    "complete_pools_df[\"imbalance\"] = complete_pools_df[\"volume_in\"] - complete_pools_df[\"volume_out\"]\n",
    "\n",
    "#Adding gas fees to dataframe\n",
    "complete_pools_df = pd.merge(complete_pools_df, average_daily_gas_fees, on=['date'])\n",
    "\n",
    "#Adding liquidity tokens to dataframe\n",
    "complete_pools_df = pd.merge(complete_pools_df, complete_pool_tokens_df, on=['pool', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the total pool value (=pool size) for each day\n",
    "def calculate_total_pool_value(prices_df, pools_df):\n",
    "    prices_df_reversed = prices_df.copy()\n",
    "    prices_df_reversed.rename(columns={\n",
    "        'token_bought_symbol': 'token_sold_symbol', \n",
    "        'token_sold_symbol': 'token_bought_symbol',\n",
    "        'avg_usd_price_token_bought': 'avg_usd_price_token_sold',\n",
    "        'avg_usd_price_token_sold': 'avg_usd_price_token_bought'\n",
    "    }, inplace=True)\n",
    "\n",
    "    prices_df_combined = pd.concat([prices_df, prices_df_reversed])\n",
    "\n",
    "    merged_df = pd.merge(pools_df, prices_df_combined, \n",
    "                         left_on=['date', 'symbol_1', 'symbol_2'], \n",
    "                         right_on=['date', 'token_bought_symbol', 'token_sold_symbol'], \n",
    "                         how='left')\n",
    "\n",
    "    merged_df['USD_value_symbol_1'] = merged_df['x'] * merged_df['avg_usd_price_token_bought']\n",
    "    merged_df['USD_value_symbol_2'] = merged_df['y'] * merged_df['avg_usd_price_token_sold']\n",
    "    merged_df['total_USD_value'] = merged_df['USD_value_symbol_1'] + merged_df['USD_value_symbol_2']\n",
    "    merged_df = merged_df.rename({'pool_x': 'pool'}, axis=1)\n",
    "    merged_df = merged_df[['pool', 'date', 'total_USD_value']]\n",
    "    merged_df = merged_df.drop_duplicates(subset=['pool', 'date'])\n",
    "    return merged_df\n",
    "\n",
    "#Calculate for Uniswap\n",
    "Uniswap_merged_df = calculate_total_pool_value(Uniswap_prices_df, Uniswap_pools_df)\n",
    "\n",
    "#Calculate for Sushiswap\n",
    "Sushiswap_merged_df = calculate_total_pool_value(Sushiswap_prices_df, Sushiswap_pools_df)\n",
    "\n",
    "#Concatenate both dataframes\n",
    "final_df = pd.concat([Uniswap_merged_df, Sushiswap_merged_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3031fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging pool size with the complete dataframe\n",
    "complete_pools_df = pd.merge(final_df, complete_pools_df, on=['pool', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a categorical variable that declares the pool type\n",
    "#Clearly specifying exotic and stable pairs and adding rest to normal pairs\n",
    "pair_categories = {\n",
    "    'WETH-AMPL': 'Exotic',\n",
    "    'USDC-USDT': 'Stable',\n",
    "    'DAI-USDC': 'Stable',\n",
    "    'DAI-USDT': 'Stable',\n",
    "    'PICKLE-WETH': 'Exotic',\n",
    "    'FARM-WETH': 'Exotic',\n",
    "    'CREAM-WETH': 'Exotic',\n",
    "     'CORE-WETH': 'Exotic'\n",
    "}\n",
    "\n",
    "def assign_category(row):\n",
    "    pair = row['symbol_1'] + '-' + row['symbol_2']\n",
    "    return pair_categories.get(pair, 'Normal')\n",
    "\n",
    "complete_pools_df['pool_category'] = complete_pools_df.apply(assign_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42baaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting DEX flags to dtype int\n",
    "complete_pools_df['Uniswap'] = complete_pools_df['Uniswap'].astype('int')\n",
    "complete_pools_df['Sushiswap'] = complete_pools_df['Sushiswap'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating profits of liquidity provision on both dex across cross-section of pairs\n",
    "bom_usd_prices_dataframes = {\n",
    "    \"Uniswap\": bom_usd_prices_Uniswap, \n",
    "    \"Sushiswap\": bom_usd_prices_Sushiswap\n",
    "}\n",
    "\n",
    "complete_pools_Uniswap_df = complete_pools_df[complete_pools_df['Uniswap'] == 1]\n",
    "complete_pools_Sushiswap_df = complete_pools_df[complete_pools_df['Sushiswap'] == 1]\n",
    "\n",
    "complete_pools_Uniswap_df.set_index('date', inplace=True)\n",
    "complete_pools_Sushiswap_df.set_index('date', inplace=True)\n",
    "complete_pools_df = complete_pools_df.set_index(\"date\")\n",
    "\n",
    "#How much of the pools total liquidity to be invested\n",
    "investment_percentages = [0.01]  \n",
    "\n",
    "#timeframe of observation (equals ten three month periods)\n",
    "start_date = pd.to_datetime('2020-10-01')\n",
    "end_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "#Calculating profit for each DEX\n",
    "profit_results = []\n",
    "\n",
    "for platform, complete_pools_df in [(\"Uniswap\", complete_pools_Uniswap_df), (\"Sushiswap\", complete_pools_Sushiswap_df)]:\n",
    "    bom_usd_prices = bom_usd_prices_dataframes[platform]\n",
    "\n",
    "    start_date = pd.to_datetime('2020-10-01')\n",
    "    end_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date = start_date + pd.DateOffset(months=1)\n",
    "\n",
    "        for pool in complete_pools_df['pool'].unique():\n",
    "            pool_data = complete_pools_df[complete_pools_df['pool'] == pool]\n",
    "\n",
    "            pool_data_monthly = pool_data.resample('MS').first()\n",
    "\n",
    "            token_bought_symbol = pool_data['symbol_1'].iloc[0]\n",
    "            token_sold_symbol = pool_data['symbol_2'].iloc[0]\n",
    "\n",
    "            bom_prices_token_bought = bom_usd_prices[token_bought_symbol]\n",
    "            bom_prices_token_sold = bom_usd_prices[token_sold_symbol]\n",
    "\n",
    "            #Calculating USD pool value of each token in a pool at timepoint t_1 and t_2\n",
    "            pool_value_t1_token_bought = pool_data_monthly.loc[start_date, 'x'] * bom_prices_token_bought.loc[start_date]\n",
    "            pool_value_t1_token_sold = pool_data_monthly.loc[start_date, 'y'] * bom_prices_token_sold.loc[start_date]\n",
    "            pool_value_t2_token_bought = pool_data_monthly.loc[next_date, 'x'] * bom_prices_token_bought.loc[next_date]\n",
    "            pool_value_t2_token_sold = pool_data_monthly.loc[next_date, 'y'] * bom_prices_token_sold.loc[next_date]\n",
    "\n",
    "            #Calculating the total pool value by summing both\n",
    "            total_pool_value_t1 = pool_value_t1_token_bought + pool_value_t1_token_sold\n",
    "            total_pool_value_t2 = pool_value_t2_token_bought + pool_value_t2_token_sold\n",
    "\n",
    "            #Getting amount of pool tokens at t_1 and t_2\n",
    "            lp_tokens_t1 = pool_data_monthly.loc[start_date, 'pool_token_amount']\n",
    "            lp_tokens_t2 = pool_data_monthly.loc[next_date, 'pool_token_amount']\n",
    "            \n",
    "            for investment_percentage in investment_percentages:\n",
    "                #Calculating how much is invested in terms of USD at t_1\n",
    "                invest_t1 = investment_percentage * total_pool_value_t1\n",
    "                #Calculating the amount of liquidity tokens a liq. providers gets in t_1 based on the investment amount\n",
    "                amount0 = investment_percentage * pool_data_monthly.loc[start_date, 'x']\n",
    "                amount1 = investment_percentage * pool_data_monthly.loc[start_date, 'y']\n",
    "                _totalSupply = pool_data_monthly.loc[start_date, 'pool_token_amount']\n",
    "                _reserve0 = pool_data_monthly.loc[start_date, 'x']\n",
    "                _reserve1 = pool_data_monthly.loc[start_date, 'y']\n",
    "                liquidity_t1 = min(amount0 * _totalSupply / _reserve0, amount1 * _totalSupply / _reserve1)\n",
    "\n",
    "                #Calculating how much (both absolute and relative) of the liquidity tokens the same LP owns in t_2\n",
    "                lp_tokens_owned = liquidity_t1\n",
    "                percentage_t2 = lp_tokens_owned / lp_tokens_t2\n",
    "                #Based on this calculating the USD value of his liq. position\n",
    "                invest_t2 = percentage_t2 * total_pool_value_t2\n",
    "                #Calculating the return of just holding the tokens\n",
    "                hold_t1 = invest_t1 * (bom_prices_token_bought.loc[start_date] + bom_prices_token_sold.loc[start_date])\n",
    "                hold_t2 = invest_t2 * (bom_prices_token_bought.loc[next_date] + bom_prices_token_sold.loc[next_date])\n",
    "                \n",
    "                #Calculating the return of providing liquidity vs holding the token\n",
    "                return_value = 100 * ((invest_t2 / hold_t1) - (invest_t1 / hold_t1)) / (invest_t1 / hold_t1)\n",
    "\n",
    "                #Appending returns and other variables to dict\n",
    "                profit_results.append({\n",
    "                    'platform': platform,\n",
    "                    'pool': pool,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': next_date,\n",
    "                    'return': return_value\n",
    "                })\n",
    "\n",
    "        start_date = next_date\n",
    "\n",
    "#Converting the dict to a dataframe\n",
    "profit_df = pd.DataFrame(profit_results)\n",
    "print(profit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b43646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the two pool dfs together again, as they get split in previous codeblock\n",
    "complete_pools_df = pd.concat([complete_pools_Uniswap_df, complete_pools_Sushiswap_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding token symbols to contract addresses (for identification)\n",
    "def get_token_symbols(pool_address):\n",
    "    return pool_mapping_Uniswap.get(pool_address, ('Unknown', 'Unknown'))\n",
    "profit_df['symbol_1'], profit_df['symbol_2'] = zip(*profit_df['pool'].map(get_token_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting profits for both pools\n",
    "#Splitting data by DEX\n",
    "profit_df_Uniswap = profit_df[profit_df['platform'] == 'Uniswap']\n",
    "profit_df_Sushiswap = profit_df[profit_df['platform'] == 'Sushiswap']\n",
    "\n",
    "#Uniswap plot\n",
    "traces_Uniswap = []\n",
    "for pool in profit_df_Uniswap['pool'].unique():\n",
    "    pool_data = profit_df_Uniswap[profit_df_Uniswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        name=f'{pool} Pool'\n",
    "    )\n",
    "    traces_Uniswap.append(trace)\n",
    "\n",
    "layout_Uniswap = go.Layout(\n",
    "    title='Uniswap one-month pool profit over time',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Profit (%)')\n",
    ")\n",
    "\n",
    "fig_Uniswap = go.Figure(data=traces_Uniswap, layout=layout_Uniswap)\n",
    "\n",
    "#Sushiswap plot\n",
    "traces_Sushiswap = []\n",
    "for pool in profit_df_Sushiswap['pool'].unique():\n",
    "    pool_data = profit_df_Sushiswap[profit_df_Sushiswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        name=f'{pool} Pool'\n",
    "    )\n",
    "    traces_Sushiswap.append(trace)\n",
    "\n",
    "layout_Sushiswap = go.Layout(\n",
    "    title='Sushiswap one-month pool profit over time',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Profit (%)')\n",
    ")\n",
    "\n",
    "fig_Sushiswap = go.Figure(data=traces_Sushiswap, layout=layout_Sushiswap)\n",
    "\n",
    "offline.init_notebook_mode(connected=True)\n",
    "offline.plot(fig_Uniswap, filename='Uniswap_profit_plot.html', auto_open=False)\n",
    "offline.plot(fig_Sushiswap, filename='Sushiswap_profit_plot.html', auto_open=False)\n",
    "offline.iplot(fig_Uniswap)\n",
    "offline.iplot(fig_Sushiswap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d497db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Same but next to each other\n",
    "\n",
    "traces_Uniswap = []\n",
    "for pool in profit_df_Uniswap['pool'].unique():\n",
    "    pool_data = profit_df_Uniswap[profit_df_Uniswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    traces_Uniswap.append(trace)\n",
    "\n",
    "traces_Sushiswap = []\n",
    "for pool in profit_df_Sushiswap['pool'].unique():\n",
    "    pool_data = profit_df_Sushiswap[profit_df_Sushiswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    traces_Sushiswap.append(trace)\n",
    "\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=('Uniswap', 'Sushiswap'))\n",
    "\n",
    "for trace in traces_Uniswap:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in traces_Sushiswap:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"One-month pool returns over analysis timeframe\")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=2)\n",
    "\n",
    "offline.init_notebook_mode(connected=True)\n",
    "offline.plot(fig, filename='combined_profit_plot.html', auto_open=False)\n",
    "offline.iplot(fig)\n",
    "\n",
    "pio.write_image(fig, '/Users/fabioza/Desktop/Master thesis/sushi_uni_1m_returns.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pools_df = complete_pools_df.reset_index()\n",
    "\n",
    "#Calculating daily returns, based on returns the 30d vola and impermanent loss for each pair in the liquidity pool\n",
    "complete_pools_df['date'] = pd.to_datetime(complete_pools_df['date'])\n",
    "complete_pools_df = complete_pools_df.sort_values(['pool', 'date'])\n",
    "complete_pools_df['daily_return'] = complete_pools_df.groupby('pool')['avg_swap_price'].pct_change()\n",
    "\n",
    "#Vola as 30d stdev of daily returns\n",
    "complete_pools_df['30D_volatility'] = complete_pools_df.groupby('pool')['daily_return'].rolling(window=30).std().reset_index(0, drop=True)\n",
    "\n",
    "#impermanent loss as the percentage change of the ratio between the two tokens in the pool btw. time t_1 and t_2 \n",
    "def impermanent_loss(pt1, pt2):\n",
    "    ratio = pt2 / pt1\n",
    "    return 100 * ((2 * np.sqrt(ratio)) / (1 + ratio) - 1)\n",
    "\n",
    "complete_pools_df['pt1'] = complete_pools_df.groupby('pool')['avg_swap_price'].shift(29)\n",
    "complete_pools_df['30D_impermanent_loss'] = complete_pools_df.apply(lambda x: impermanent_loss(x['pt1'], x['avg_swap_price']) if pd.notnull(x['pt1']) else np.nan, axis=1)\n",
    "\n",
    "complete_pools_df = complete_pools_df.drop(columns=['pt1'])\n",
    "complete_pools_df = complete_pools_df.drop(columns=['daily_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pools_df = complete_pools_df.set_index('date')\n",
    "\n",
    "#Resampling and aggregating features on a 1-month horizon\n",
    "df_resampled = complete_pools_df.groupby('pool').resample('1M').agg({\n",
    "    'volume_usd': 'sum',\n",
    "    'imbalance': 'mean',\n",
    "    'avg_gas': 'mean',\n",
    "    '30D_volatility': 'last',\n",
    "    '30D_impermanent_loss': 'last',\n",
    "    'pool_category': 'first',\n",
    "    'total_USD_value': 'first',\n",
    "    'Uniswap': 'first',\n",
    "    'Sushiswap': 'first'\n",
    "})\n",
    "\n",
    "df_resampled.reset_index(inplace=True)\n",
    "\n",
    "#Making sure that date capture the first day of the month\n",
    "df_resampled['date'] = df_resampled['date'] + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "df_resampled.rename(columns={'date': 'start_date'}, inplace=True)\n",
    "\n",
    "#Adding profits (the feature we want to predict)\n",
    "df_final = pd.merge(df_resampled, profit_df, on=['pool', 'start_date'])\n",
    "\n",
    "#As we want to predict the return in one month from now we need to introduce this as a feature\n",
    "df_final['return_in_1month'] = df_final.groupby('pool')['return'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For two observations total_USD_value contains inf, I fill up this observation with the average of the observation before it and after it. This shouldn't bias results\n",
    "pool_values = [\"0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc\", \"0x0d4a11d5eeaac28ec3f61d100daf4d40471f1852\"]\n",
    "\n",
    "for pool_value in pool_values:\n",
    "    pool_rows = df_final[df_final[\"pool\"] == pool_value]\n",
    "    filled_values = pool_rows[\"total_USD_value\"].rolling(3, min_periods=1, center=True).mean()\n",
    "    df_final.loc[df_final[\"pool\"] == pool_value, \"total_USD_value\"] = filled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data by date and then by pool for tree based models\n",
    "df_final = df_final.sort_values(by=['start_date', 'pool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ded509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a separate dataframe for recurrent networks\n",
    "df_final_RNN = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "df_final = df_final.drop(columns=['symbol_1', 'symbol_2', 'platform', 'start_date', 'end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping na values\n",
    "df_final = df_final.dropna()\n",
    "df_final_RNN = df_final_RNN.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff8dbf",
   "metadata": {},
   "source": [
    "## Summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1816612",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_df_analysis = profit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09616ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning pool categories to pool contract addresses\n",
    "profit_df_analysis['pool_category'] = profit_df_analysis.apply(assign_category, axis=1)\n",
    "\n",
    "#Grouping by pool category and counting positive and negative returns and calculating stdev\n",
    "results = profit_df_analysis.groupby('pool_category')['return'].agg(\n",
    "    positive_count = lambda x: (x > 0).sum(),\n",
    "    negative_count = lambda x: (x < 0).sum(),\n",
    "    std_dev = 'std'\n",
    ")\n",
    "\n",
    "#Calculating ratios per category\n",
    "results['positive_ratio'] = results['positive_count'] / (results['positive_count'] + results['negative_count'])\n",
    "results['negative_ratio'] = results['negative_count'] / (results['positive_count'] + results['negative_count'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57647c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive vs. negative returns before forecasting / testing period\n",
    "filtered_df = df_final_RNN[df_final_RNN[\"start_date\"] < \"2022-08-01\"]\n",
    "\n",
    "num_positives = filtered_df[filtered_df[\"return_in_1month\"] > 0].shape[0]\n",
    "num_negatives = filtered_df[filtered_df[\"return_in_1month\"] < 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive returns: {num_positives}\")\n",
    "print(f\"Number of negative returns: {num_negatives}\")\n",
    "print(f\"Ratio pos.: {num_positives / (num_positives+num_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive vs. negative returns during forecasting / testing period\n",
    "filtered_df = df_final_RNN[df_final_RNN[\"start_date\"] > \"2022-08-01\"]\n",
    "\n",
    "num_positives = filtered_df[filtered_df[\"return_in_1month\"] > 0].shape[0]\n",
    "num_negatives = filtered_df[filtered_df[\"return_in_1month\"] < 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive returns: {num_positives}\")\n",
    "print(f\"Number of negative returns: {num_negatives}\")\n",
    "print(f\"Ratio neg. to pos.: {num_negatives}\")\n",
    "print(f\"Ratio pos.: {num_positives / (num_positives+num_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General descriptive stats\n",
    "numerical_cols = ['volume_usd', 'imbalance', 'avg_gas', '30D_volatility', '30D_impermanent_loss', 'total_USD_value', 'return', 'return_in_1month']\n",
    "\n",
    "df_final[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive stats per pool category\n",
    "df_final.groupby('pool_category')[numerical_cols].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='30D_volatility', y='return', hue='pool_category', data=df_final)\n",
    "plt.xlabel('30-days volatility') \n",
    "plt.ylabel('Return') \n",
    "plt.title('Return vs. volatility for different pool types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648753ee",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66081eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling and aggregating features on a 3-month horizon\n",
    "df_resampled = complete_pools_df.groupby('pool').resample('1M').agg({\n",
    "    'volume_usd': 'sum',\n",
    "    'imbalance': 'mean',\n",
    "    'avg_gas': 'mean',\n",
    "    '30D_volatility': 'last',\n",
    "    '30D_impermanent_loss': 'last',\n",
    "    'pool_category': 'first',\n",
    "    'total_USD_value': 'first',\n",
    "    'Uniswap': 'first',\n",
    "    'Sushiswap': 'first'\n",
    "})\n",
    "\n",
    "df_resampled.reset_index(inplace=True)\n",
    "\n",
    "#Making sure that date capture the first day of the month\n",
    "df_resampled['date'] = df_resampled['date'] + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "df_resampled.rename(columns={'date': 'start_date'}, inplace=True)\n",
    "\n",
    "#Adding profits (the feature we want to predict)\n",
    "df_final_linear = pd.merge(df_resampled, profit_df, on=['pool', 'start_date'])\n",
    "\n",
    "#As we want to predict the return in three months from now we need to introduce this as a feature\n",
    "df_final_linear['return_in_1month'] = df_final_linear.groupby('pool')['return'].shift(-1)\n",
    "\n",
    "#The last observation for each pool is empty as there is no return in three months, this observation has to be dropped\n",
    "df_final_linear = df_final_linear.dropna(subset=['return_in_1month'])\n",
    "\n",
    "#Dropping first row for each pool which now has NaN values\n",
    "df_final_linear.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preprocessing the data and getting it into the rigth format\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_final_linear[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final_linear[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Defining the features and the target\n",
    "df_final_linear = df_final_linear.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded_linear = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    df_final_linear.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded_linear['total_USD_value'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "col_index = df_encoded_linear.columns.get_loc('total_USD_value')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "total_usd_value = df_encoded_linear.iloc[:, col_index].values.reshape(-1, 1)\n",
    "df_encoded_linear.iloc[:, col_index] = imputer.fit_transform(total_usd_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "df_encoded_linear = df_encoded_linear.drop(columns = [\"symbol_1\", \"symbol_2\", \"end_date\", \"platform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting df to csv and saving locally\n",
    "df_encoded_linear.to_csv(\"/Users/fabioza/Desktop/Master thesis/data/df_encoded_linear30d.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code continued in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae70a6",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preprocessing the data and getting it into the rigth format to avoid leakage due to time-series nature of data\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Defining the features and the target\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category', 'return_in_1month'])\n",
    "y = df_no_missing['return_in_1month']\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_encoded_filled = pd.DataFrame(imputer.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "\n",
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "#According to this 4 3-month periods are used for training, 2 for validation and 2 for testing\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "#Calculate the sizes based on the proportions\n",
    "num_samples = len(df_encoded_filled)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded_filled[:train_samples]\n",
    "X_val = df_encoded_filled[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded_filled[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Initializing the random forest model\n",
    "model = RandomForestRegressor(random_state=63)\n",
    "\n",
    "#Defining parameter grid (number of trees and depth of each tree)\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 1000),  \n",
    "    'max_depth': (1, 20),\n",
    "}\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series anture of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to six observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "#Expanding window is being used, which means that in the first iteration the first validation fold is being forecasted and\n",
    "#in the second run, it is being added to the training set and the last validation fold is being forecasted\n",
    "window_length = 2\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "        \n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee6e06",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78db3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with Gradient Boosting with grid search and bayesian optimization\n",
    "#Best parameters with hyperparam search are found based on train / validation set\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#Defining target and features\n",
    "y = df_no_missing['return_in_1month']\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "X = df_encoded.drop(columns=['return_in_1month'])\n",
    "\n",
    "#Initializing the gradient boosting model, adding regularization and early stopping to avoid overfitting the model\n",
    "model = GradientBoostingRegressor(min_samples_split = 10, min_samples_leaf = 4, \n",
    "                                  random_state=63, max_depth=3, max_features='sqrt', \n",
    "                                  subsample=0.8, n_iter_no_change=5, tol=0.01)\n",
    "\n",
    "#Defining parameter grid (number of trees, depth of each tree and learning rate)\n",
    "param_grid = {\n",
    "    'n_estimators': (1, 100),   \n",
    "    'max_depth': (1, 5),          \n",
    "    'learning_rate': (0.001, 0.1)   \n",
    "}\n",
    "\n",
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "train_size = 0.8 \n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "#Calculate the sizes based on the proportions\n",
    "num_samples = len(df_encoded)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded[:train_samples]\n",
    "X_val = df_encoded[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series anture of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to six observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "#Expanding window is being used, which means that in the first iteration the first validation fold is being forecasted and\n",
    "#in the second run, it is being added to the training set and the last validation fold is being forecasted\n",
    "window_length = 2\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    \n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cce6b",
   "metadata": {},
   "source": [
    "## Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One with Bayesian Optimization (SGD) and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_1month']))\n",
    "y = df_encoded['return_in_1month']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        #DNN has one hidden layer, a batch normalization layer, an output layer and a dropout layer\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) \n",
    "        self.fc2 = nn.Linear(64, 1)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    #Different optimizers such as adam or adagrad have been tested out. SGD performs the best\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        #Splitting up the data into training and validation features + labels\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        #Converting training and validation data to tensor\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))\n",
    "    \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Three with Bayesian Optimization (SGD) and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)  \n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_1month']))\n",
    "y = df_encoded['return_in_1month']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        #DNN has three hidden layers, three batch normalization layer, an output layer and a dropout layer\n",
    "        self.fc1 = nn.Linear(X.shape[1], 256)  \n",
    "        self.bn1 = nn.BatchNorm1d(256) \n",
    "        self.fc2 = nn.Linear(256, 128) \n",
    "        self.bn2 = nn.BatchNorm1d(128)  \n",
    "        self.fc3 = nn.Linear(128, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)   \n",
    "        self.fc4 = nn.Linear(64, 1)     \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))   \n",
    "        \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five with Bayesian Optimization (SGD) and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)  \n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_1month']))\n",
    "y = df_encoded['return_in_1month']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        #DNN has five hidden layers, five batch normalization layer, an output layer and a dropout layer\n",
    "        self.fc1 = nn.Linear(X.shape[1], 256)  \n",
    "        self.bn1 = nn.BatchNorm1d(256)  \n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.bn2 = nn.BatchNorm1d(128)  \n",
    "        self.fc3 = nn.Linear(128, 64)  \n",
    "        self.bn3 = nn.BatchNorm1d(64)  \n",
    "        self.fc4 = nn.Linear(64, 32)  \n",
    "        self.bn4 = nn.BatchNorm1d(32)  \n",
    "        self.fc5 = nn.Linear(32, 16)  \n",
    "        self.bn5 = nn.BatchNorm1d(16)  \n",
    "        self.fc6 = nn.Linear(16, 1) \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))   \n",
    "        \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942eb9e",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for recurrent neural networks\n",
    "df_no_missing = df_final_RNN.dropna()\n",
    "\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_no_missing.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded['total_USD_value'].fillna(df_encoded['total_USD_value'].mean(), inplace=True)\n",
    "\n",
    "#Dropping unnecessary columns\n",
    "df_encoded = df_encoded.drop(columns = [\"platform\", \"end_date\", \"symbol_1\", \"symbol_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885660b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out a GRU model\n",
    "#Code to create sequences from the input data\n",
    "def create_sequences(input_data, labels, seq_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "\n",
    "    for i in range(len(input_data) - seq_length + 1):\n",
    "        sequence = input_data[i:i+seq_length].values  \n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)  \n",
    "        sequences.append(sequence)\n",
    "        seq_labels.append(labels[i+seq_length-1])\n",
    "\n",
    "    return torch.stack(sequences), torch.tensor(seq_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Split data into train and test sets (75% of data is used for training, 25% for testing)\n",
    "train_size = int(0.75 * len(df_encoded))\n",
    "test_size = len(df_encoded) - train_size\n",
    "\n",
    "train_data = df_encoded[:train_size]\n",
    "test_data = df_encoded[train_size:]\n",
    "\n",
    "train_data = train_data.sort_values(by=['pool', 'start_date'])\n",
    "test_data = test_data.sort_values(by=['pool', 'start_date'])\n",
    "\n",
    "train_data = train_data.drop(columns = ['pool', 'start_date'])\n",
    "test_data = test_data.drop(columns = ['pool', 'start_date'])\n",
    "\n",
    "#The longer the more historic info taken into account but also the more complex training\n",
    "seq_length = 2\n",
    "\n",
    "#Bring input data for each pool into the rigth format (there's 35 pools with 24 observations each)\n",
    "train_sequences = []\n",
    "train_labels = []\n",
    "for i in range(35):\n",
    "    pool_df = train_data[i*18:(i+1)*18].reset_index(drop=True)\n",
    "    pool_inputs, pool_labels = create_sequences(pool_df.drop(columns=['return_in_1month']), \n",
    "                                                pool_df['return_in_1month'], seq_length)\n",
    "    train_sequences.append(pool_inputs)\n",
    "    train_labels.append(pool_labels)\n",
    "    \n",
    "seq_length = 1\n",
    "\n",
    "#Same for test data\n",
    "test_sequences = []\n",
    "test_labels = []\n",
    "for i in range(35):\n",
    "    pool_df = test_data[i*6:(i+1)*6].reset_index(drop=True)\n",
    "    pool_inputs, pool_labels = create_sequences(pool_df.drop(columns=['return_in_1month']), \n",
    "                                                pool_df['return_in_1month'], seq_length)\n",
    "    test_sequences.append(pool_inputs)\n",
    "    test_labels.append(pool_labels)\n",
    "\n",
    "#Converting rrain_sequences and test_sequences to lists of tensors\n",
    "train_sequences = torch.cat(train_sequences)\n",
    "train_labels = torch.cat(train_labels)\n",
    "test_sequences = torch.cat(test_sequences)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n",
    "#Converting training and validation data to tensor form\n",
    "train_data = TensorDataset(train_sequences, train_labels)\n",
    "test_data = TensorDataset(test_sequences, test_labels)\n",
    "\n",
    "#Provides model with batches of training and validation data \n",
    "#No shuffling to preserve time series nature of data\n",
    "trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "testloader = DataLoader(test_data, batch_size=30, shuffle=False)\n",
    "\n",
    "#Defining model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "#Setting up time-series cross val\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def train_model(lr: float, weight_decay: float, hidden_dim: int, n_layers: int, dropout_rate: float):\n",
    "    #To return the best model\n",
    "    global best_model\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    n_layers = int(n_layers)\n",
    "    model = Net(input_dim=train_sequences.shape[2], hidden_dim=hidden_dim, n_layers=n_layers, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "    best_score = np.inf\n",
    "    \n",
    "    best_model = None\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, hit_rates = [], [], [], []\n",
    "\n",
    "    for train_index, val_index in tscv.split(train_sequences):\n",
    "        train_split = train_sequences[train_index]\n",
    "        val_split = train_sequences[val_index]\n",
    "\n",
    "        train_split_labels = train_labels[train_index]\n",
    "        val_split_labels = train_labels[val_index]\n",
    "\n",
    "        train_data = TensorDataset(train_split, train_split_labels)\n",
    "        val_data = TensorDataset(val_split, val_split_labels)\n",
    "\n",
    "        trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "        valloader = DataLoader(val_data, batch_size=30, shuffle=False)\n",
    "\n",
    "        model.train()\n",
    "        epochs = 1000\n",
    "        best_loss = np.inf\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "                    mse = mean_squared_error(labels.detach().numpy(), output.detach().numpy())\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(labels.detach().numpy(), output.detach().numpy())\n",
    "                    hit_rate = np.mean((np.sign(labels.detach().numpy()) == np.sign(output.detach().numpy())))\n",
    "\n",
    "                    mse_scores.append(mse)\n",
    "                    rmse_scores.append(rmse)\n",
    "                    mae_scores.append(mae)\n",
    "                    hit_rates.append(hit_rate)\n",
    "\n",
    "            scheduler.step(val_running_loss)\n",
    "\n",
    "            if val_running_loss < best_loss:\n",
    "                best_loss = val_running_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        if best_loss < best_score:\n",
    "            best_score = best_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "\n",
    "    return -best_score\n",
    "\n",
    "#Defining grid for hyperparam search with bayesian opt\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'hidden_dim': (50, 500),\n",
    "    'n_layers': (1, 3),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Defining bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "#Optimizer runs 25 times, changing three params each\n",
    "optimizer.maximize(\n",
    "    init_points=15,\n",
    "    n_iter= 1\n",
    ")\n",
    "\n",
    "print(optimizer.max)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f873f",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out a LSTM model\n",
    "#Code to create sequences from the input data\n",
    "def create_sequences(input_data, labels, seq_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "\n",
    "    for i in range(len(input_data) - seq_length + 1):\n",
    "        sequences.append(input_data[i:i+seq_length])  \n",
    "        seq_labels.append(labels[i+seq_length-1])  \n",
    "\n",
    "    return torch.stack(sequences), torch.tensor(seq_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#The longer the more historic info taken into account but also the more complex training\n",
    "seq_length = 6\n",
    "\n",
    "#Bring input data for each pool into the rigth format (there's 35 pools with 8 observations each)\n",
    "pool_data = []\n",
    "for i in range(35):\n",
    "    #Pool_df contains observations on each pool\n",
    "    pool_df = df_encoded[i*8:(i+1)*8]\n",
    "    #Extracting features\n",
    "    pool_inputs = torch.tensor(pool_df.drop(columns=['return_in_1month']).values, dtype=torch.float32)\n",
    "    #Extracting target\n",
    "    pool_labels = torch.tensor(pool_df['return_in_1month'].values, dtype=torch.float32)\n",
    "    #Creating the sequences of features and target\n",
    "    pool_sequences, pool_seq_labels = create_sequences(pool_inputs, pool_labels, seq_length)\n",
    "    pool_data.append((pool_sequences, pool_seq_labels))\n",
    "\n",
    "#Concat data from all pools\n",
    "inputs = torch.cat([data[0] for data in pool_data])\n",
    "labels = torch.cat([data[1] for data in pool_data])\n",
    "\n",
    "#Split data into train and validation sets (75% of data is used for training, 25% for validation)\n",
    "#We would also need to define a test set but performance is very far off from other models, which means we wouldn't use this model anywaystrain_size = int(0.75 * len(inputs))\n",
    "train_sequences, train_labels = inputs[:train_size], labels[:train_size]\n",
    "val_sequences, val_labels = inputs[train_size:], labels[train_size:]\n",
    "\n",
    "#Converting training and validation data to tensor form\n",
    "train_data = TensorDataset(train_sequences, train_labels)\n",
    "val_data = TensorDataset(val_sequences, val_labels)\n",
    "\n",
    "#Provides model with batches of training and validation data \n",
    "#No shuffling to preserve time series nature of data\n",
    "trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "valloader = DataLoader(val_data, batch_size=30, shuffle=False)\n",
    "\n",
    "#Defining model architecture\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_model(lr, weight_decay, hidden_dim, n_layers, dropout_rate):\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    n_layers = int(n_layers)\n",
    "    model = LSTMNet(input_dim=train_sequences.shape[2], hidden_dim=hidden_dim, n_layers=n_layers, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "    #Training the model\n",
    "    epochs = 1000\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        scheduler.step(val_running_loss)\n",
    "\n",
    "        #Early stopping if no further progress is being made in training\n",
    "        if val_running_loss < best_loss:\n",
    "            best_loss = val_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "    #negative loss is metric we want to maximize (want to minimize loss)\n",
    "    return -best_loss\n",
    "\n",
    "#Defining grid for hyperparam search with bayesian opt\n",
    "bounds = {\n",
    "    'lr': (1e-5, 1e-2),\n",
    "    'weight_decay': (1e-6, 1e-3),\n",
    "    'hidden_dim': (32, 128),\n",
    "    'n_layers': (1, 5),\n",
    "    'dropout_rate': (0.0, 0.7)\n",
    "}\n",
    "\n",
    "#Defining bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#Optimizer runs 50 times, changing five params each\n",
    "optimizer.maximize(init_points=5, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af247f5",
   "metadata": {},
   "source": [
    "## Re-training the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with Gradient Boosting with grid search and bayesian optimization\n",
    "#Best parameters with hyperparam search are found based on train / validation set and performance of best model is then evaluated on test set\n",
    "#This is being done as gradient boosting has best performance on training / validation and is therefore best model overall\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#df_no_missing = df_no_missing.drop(columns = [\"platform\", \"symbol_1\", \"symbol_2\", \"start_date\", \"end_date\"])\n",
    "\n",
    "#Defining target and features\n",
    "y = df_no_missing['return_in_1month']\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "X = df_encoded.drop(columns=['return_in_1month'])\n",
    "\n",
    "#Initializing the gradient boosting model, adding regularization and early stopping to avoid overfitting the model\n",
    "model = GradientBoostingRegressor(min_samples_split = 10, min_samples_leaf = 4, \n",
    "                                  random_state=63, max_depth=3, max_features='sqrt', \n",
    "                                  subsample=0.8, n_iter_no_change=5, tol=0.01)\n",
    "\n",
    "#Defining parameter grid (number of trees, depth of each tree and learning rate)\n",
    "param_grid = {\n",
    "    'n_estimators': (1, 100),   \n",
    "    'max_depth': (1, 5),          \n",
    "    'learning_rate': (0.001, 0.1)   \n",
    "}\n",
    "\n",
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "#According to this 4 3-month periods are used for training, 2 for validation and 2 for testing\n",
    "train_size = 0.5\n",
    "val_size = 0.25\n",
    "test_size = 0.25\n",
    "\n",
    "#Calculating sizes based on the proportions\n",
    "num_samples = len(df_encoded)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded[:train_samples]\n",
    "X_val = df_encoded[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series nature of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to 18 observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=18)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "fold_num = 1\n",
    "\n",
    "window_length = 6\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "#Using a k-fold cross validation approach\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    \n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "        fold_num += 1\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")\n",
    "\n",
    "plt.plot(range(1, fold_num), mae_list, marker='o')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Mean Absolute Error for Each Fold')\n",
    "plt.show()\n",
    "\n",
    "actual_values = []\n",
    "predicted_values = []\n",
    "lower_values = []\n",
    "upper_values = [] \n",
    "\n",
    "#Splitting data into train_val and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=test_samples, shuffle=False)\n",
    "\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)\n",
    "\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "#Adding a period column to the test set to apply a expanding window approach\n",
    "for i in range(pool_encoded.shape[1]):\n",
    "    X_test_copy['pool_'+str(i)] = X_test_copy['pool_'+str(i)].cumsum()\n",
    "\n",
    "X_test_copy['period'] = X_test_copy[['pool_' + str(i) for i in range(pool_encoded.shape[1])]].max(axis=1)\n",
    "\n",
    "#Training two additional models to predict the 5th and 95th percentiles (to predict uncertainty)\n",
    "model_lower = GradientBoostingRegressor(loss='quantile', alpha=0.05, **bayes_search.best_params_)\n",
    "model_upper = GradientBoostingRegressor(loss='quantile', alpha=0.95, **bayes_search.best_params_)\n",
    "\n",
    "#Fitting best model on train_val set\n",
    "best_model.fit(X_trainval, y_trainval)\n",
    "model_lower.fit(X_trainval, y_trainval)\n",
    "model_upper.fit(X_trainval, y_trainval)\n",
    "\n",
    "periods = X_test_copy['period'].unique()\n",
    "\n",
    "#Applying walk forward approach (first predicting the first test period, then adding it to training set, refitting the model\n",
    "#and predicting the second period)\n",
    "for i in range(6):\n",
    "    X_test_copy_period = X_test_copy[X_test_copy['period'] == periods[i]]\n",
    "    X_test_period = X_test.loc[X_test_copy_period.index]\n",
    "    y_test_period = y_test.loc[X_test_copy_period.index]\n",
    "    y_test_period = y_test_period.reset_index(drop = True)\n",
    "    X_test_copy_period = X_test_copy_period.drop('period', axis=1)\n",
    "\n",
    "    #Predicting target variable in test set\n",
    "    y_pred_test = best_model.predict(X_test_period)\n",
    "    y_pred_lower = model_lower.predict(X_test_period)\n",
    "    y_pred_upper = model_upper.predict(X_test_period)\n",
    "\n",
    "    #Storing actual values, predicted values and the upper and lower bounds of the forecast\n",
    "    actual_values.extend(y_test_period)\n",
    "    predicted_values.extend(y_pred_test)\n",
    "    lower_values.extend(y_pred_lower)  \n",
    "    upper_values.extend(y_pred_upper)\n",
    "\n",
    "    #Calculating eval metrics on test set\n",
    "    mse_test = mean_squared_error(y_test_period, y_pred_test)\n",
    "    rmse_test = math.sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test_period, y_pred_test)\n",
    "    hit_rate_test = np.mean((y_test_period > 0) == (y_pred_test > 0))\n",
    "\n",
    "    print(f\"Period {periods[i]}:\")\n",
    "    print(f\"MSE on Test Set: {mse_test}\")\n",
    "    print(f\"RMSE on Test Set: {rmse_test}\")\n",
    "    print(f\"MAE on Test Set: {mae_test}\")\n",
    "    print(f\"Hit Rate on Test Set: {hit_rate_test}\")\n",
    "\n",
    "    #Adding data of the current testing period to the trainval set\n",
    "    X_trainval = pd.concat([X_trainval, X_test_period])\n",
    "    y_trainval = pd.concat([y_trainval, y_test_period])\n",
    "\n",
    "    #Refitting model on new training set\n",
    "    best_model.fit(X_trainval, y_trainval)\n",
    "    model_lower.fit(X_trainval, y_trainval)\n",
    "    model_upper.fit(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting 95% confidence interval around predictions\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "#Lineplot for actual and predicted values\n",
    "ax.plot(range(len(actual_values)), actual_values, color='blue', label='Actual Return')\n",
    "ax.plot(range(len(predicted_values)), predicted_values, color='red', label='Predicted Return')\n",
    "\n",
    "#Plotting the confidence interval around predictions\n",
    "ax.fill_between(range(len(predicted_values)), lower_values, upper_values, color='grey', alpha=.5, label='Confidence Interval')\n",
    "\n",
    "ax.set_title(\"Predicted vs Actual Returns with a 90% Confidence Interval\")\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Return (%)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importances from best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "#Get corresponding feature names\n",
    "feature_names = X_trainval.columns\n",
    "\n",
    "#Select top 10 features\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "top_indices = sorted_indices[:10]\n",
    "top_importances = sorted_importances[:10]\n",
    "top_feature_names = sorted_feature_names[:10]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(top_importances)), top_importances, tick_label=top_feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a function to decode pool information\n",
    "def decode_pool(encoded_pool_row):\n",
    "    pool_index = np.argmax(encoded_pool_row)\n",
    "    return f'Pool {pool_index}'\n",
    "\n",
    "#Defining pool colmns as they occur in df_encoded\n",
    "pool_columns = [f'pool_{i}' for i in range(35)]\n",
    "\n",
    "#Adding pool info to df\n",
    "test_dataset_df = pd.DataFrame(testset[:][0].numpy(), columns=df_encoded.columns.drop('return_in_1month'))\n",
    "test_dataset_df['pool_info'] = test_dataset_df[pool_columns].apply(decode_pool, axis=1)\n",
    "\n",
    "#Adding actual and predicted return to df\n",
    "test_dataset_df['actual_return'] = test_labels\n",
    "test_dataset_df['predicted_return'] = test_preds\n",
    "\n",
    "#Plotting actual vs predicted values for each pool\n",
    "unique_pools = test_dataset_df['pool_info'].unique()\n",
    "for pool in unique_pools:\n",
    "    pool_data = test_dataset_df[test_dataset_df['pool_info'] == pool]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pool_data['actual_return'].values, label='Actual')\n",
    "    plt.plot(pool_data['predicted_return'].values, label='Predicted')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Return in 3 Months')\n",
    "    plt.title(f'Returns for {pool}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb404d",
   "metadata": {},
   "source": [
    "## Economic inituition / optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece8774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the addresses of the pools\n",
    "pool_names = df_final[\"pool\"].unique()\n",
    "\n",
    "#List times six (six forecasting periods)\n",
    "pool_names = np.repeat(pool_names, 6)\n",
    "\n",
    "#Creating a period list (first 35 observations are in the the first period, ...)\n",
    "periods_list = [i // 35 + 1 for i in range(210)]\n",
    "\n",
    "#Adding all the relevant lists to a dataframe\n",
    "df_results = pd.DataFrame({\n",
    "    'Pool': pool_names,\n",
    "    'Period': periods_list,\n",
    "    'Actual_Return': actual_values,\n",
    "    'Predicted_Return': predicted_values,\n",
    "    'Lower bound': lower_values,\n",
    "    'Upper bound': upper_values\n",
    "})\n",
    "\n",
    "#Grouping by periods and finding the pool with the highest predicted return\n",
    "for period, group in df_results.groupby('Period'):\n",
    "    max_pred_return_pool = group.loc[group['Predicted_Return'].idxmax(), 'Pool']\n",
    "    max_pred_return = group.loc[group['Predicted_Return'].idxmax(), 'Predicted_Return']\n",
    "    actual_return = group.loc[group['Predicted_Return'].idxmax(), 'Actual_Return']\n",
    "    \n",
    "    print(f\"Period {period}:\")\n",
    "    print(f\"Pool with highest predicted return: {max_pred_return_pool}\")\n",
    "    print(f\"Predicted Return: {max_pred_return}\")\n",
    "    print(f\"Actual Return: {actual_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e57e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the case where uncertainty is not taken into account and the LP just provides liquidity for the pair with the highest return\n",
    "#Assuming a uniform distribution of gas fees of 7 USD for both Uniswap and Sushiswap (double-checked with on-chain transactions)\n",
    "#For adding liquidity the first time the LP only accounts the 7 USD once\n",
    "#For every consequent reshuffling the LP incorporates the fee for both withdrawing and adding new liquidity (two times 7 USD)\n",
    "#I arbitrarily chose three different initial sizes of liquidity positions (5'000, 50'000 and 500'000), which would approximately represent a small, large and medium LP\n",
    "\n",
    "gas_fee = 7  \n",
    "initial_position_sizes = [5000, 50000, 500000]  \n",
    "position_sizes = {size: [] for size in initial_position_sizes} \n",
    "\n",
    "#Storing important values for plotting later\n",
    "return_data = {size: {\"actual\": [], \"predicted\": [], \"upper\": [], \"lower\": []} for size in initial_position_sizes}\n",
    "\n",
    "#Looping through every investment size\n",
    "for size in initial_position_sizes:\n",
    "    current_size = size\n",
    "    #Looping through each period and selecting the pool with the highest return\n",
    "    for period, group in df_results.groupby('Period'):\n",
    "        #Pool with highest forecasted returns\n",
    "        max_pred_return_pool = group.loc[group['Predicted_Return'].idxmax(), 'Pool']\n",
    "        #Predicted return of this pool\n",
    "        max_pred_return = group.loc[group['Predicted_Return'].idxmax(), 'Predicted_Return'] / 100\n",
    "        #Actual return of this pool\n",
    "        actual_return = group.loc[group['Predicted_Return'].idxmax(), 'Actual_Return'] / 100  \n",
    "        \n",
    "        #For plotting later\n",
    "        return_data[size][\"actual\"].append(actual_return*100)\n",
    "        return_data[size][\"predicted\"].append(max_pred_return*100)\n",
    "        return_data[size][\"upper\"].append(group.loc[group['Predicted_Return'].idxmax(), 'Upper bound'])\n",
    "        return_data[size][\"lower\"].append(group.loc[group['Predicted_Return'].idxmax(), 'Lower bound'])\n",
    "        \n",
    "        #For the first period, only the cost of adding liquidity is incorporated\n",
    "        if period == 1:\n",
    "            liquidity_cost = gas_fee\n",
    "        #For every subsequent period also the cost of withdrawing the liquidity is added\n",
    "        else:\n",
    "            liquidity_cost = gas_fee * 2\n",
    "\n",
    "        #If predicted return in % * current_size > cost of adding (+withdrawing) liquidity, the LP choses to add liquidity / switch to a new pool\n",
    "        if max_pred_return * current_size > liquidity_cost:\n",
    "            #Assuming that the new positions depends on the actual return in the period and the cost of switching the pair\n",
    "            new_position_size = current_size * (1 + actual_return) - liquidity_cost \n",
    "            position_sizes[size].append(new_position_size)\n",
    "        else:\n",
    "            #If pair is not switched then the LP just gets the return of the pool in that period\n",
    "            actual_return_noswitch = df_final_RNN.loc[(df_final_RNN['start_date'] == period) & (df_final_RNN['pool'] == max_pred_return_pool), 'return'] / 100\n",
    "            #The size of the position after the period is just influenced by the return of the pool\n",
    "            new_position_size = current_size * (1 + actual_return_noswitch)\n",
    "            position_sizes[size].append(new_position_size)\n",
    "\n",
    "        current_size = new_position_size  \n",
    "\n",
    "\n",
    "for size, positions in position_sizes.items():\n",
    "    print(f\"Final position size for initial size {size}: {positions[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a020c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting actual and predicted values and confidence intervals around them\n",
    "date_mapping = {i: start_date + pd.DateOffset(months=i-1) for i in df_results['Period'].unique()}\n",
    "\n",
    "for size in return_data:\n",
    "    df = pd.DataFrame({\n",
    "        'actual_values': return_data[size]['actual'],\n",
    "        'predicted_values': return_data[size]['predicted'],\n",
    "        'upper_bound': return_data[size]['upper'],\n",
    "        'lower_bound': return_data[size]['lower']\n",
    "    }, index=[date_mapping[i] for i in range(1, len(return_data[size]['actual']) + 1)])\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df['actual_values'], color='blue', label='Actual Values')\n",
    "    plt.plot(df['predicted_values'], color='red', label='Predicted Values')\n",
    "    plt.fill_between(df.index, df['lower_bound'], df['upper_bound'], color='gray', alpha=0.2, label='Confidence Interval')\n",
    "    plt.title(f'Returns for Initial Position', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Return (%)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94058805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second optimization strategy: Sharpe Ratio is minimized\n",
    "#First period in forecasting set\n",
    "start_date = pd.to_datetime('2022-09-01')\n",
    "\n",
    "#Zero is assumed for simplicity but could also be the return for staking in a crypto context\n",
    "risk_free_rate = 0\n",
    "\n",
    "#Average gas fee for liquidity provision (both DEX and withdrawals + deposits) according to on-chain data\n",
    "gas_fee = 7\n",
    "\n",
    "#Gas fee might render certain position sizes unprofitable (not really the case with small gas fee)\n",
    "initial_position_sizes = [5000, 50000, 500000]\n",
    "position_sizes = {size: [] for size in initial_position_sizes}\n",
    "\n",
    "#Storing important values for plotting later\n",
    "return_data = {size: {\"actual\": [], \"predicted\": [], \"upper\": [], \"lower\": []} for size in initial_position_sizes}\n",
    "\n",
    "#To keep track of the pool we're currently invested in\n",
    "current_pool = {}\n",
    "\n",
    "#Looping through investment sizes\n",
    "for size in initial_position_sizes:\n",
    "    current_size = size\n",
    "    position_sizes[size] = []\n",
    "    current_pool[size] = None\n",
    "\n",
    "    #Looping through periods and pools\n",
    "    for period, group in df_results.groupby('Period'):\n",
    "        #Calculate Sharpe Ratio for each pool\n",
    "        #Note that the formula for volatility assumes symmetry around the mean / normal distribution, this is most likely not the case but is good enough for this approach\n",
    "        group['Volatility'] = (group['Upper bound'] - group['Lower bound']) / (2 * 1.96)\n",
    "        group['Excess_Return'] = group['Predicted_Return'] - risk_free_rate\n",
    "        group['Sharpe_Ratio'] = group['Excess_Return'] / group['Volatility']\n",
    "\n",
    "        #Selecting the pool with the highest Sharpe ratio\n",
    "        optimal_pool_data = group.loc[group['Sharpe_Ratio'].idxmax()]\n",
    "        \n",
    "        #For plotting later              \n",
    "        return_data[size][\"actual\"].append(optimal_pool_data['Actual_Return'])\n",
    "        return_data[size][\"predicted\"].append(optimal_pool_data['Predicted_Return'])\n",
    "        return_data[size][\"upper\"].append(optimal_pool_data['Upper bound'])\n",
    "        return_data[size][\"lower\"].append(optimal_pool_data['Lower bound'])\n",
    "        \n",
    "        #Pool with highest Sharpe ratio\n",
    "        max_sharpe_ratio_pool = optimal_pool_data['Pool']\n",
    "        #Expected return of that pool\n",
    "        expected_return = optimal_pool_data['Predicted_Return'] / 100\n",
    "        actual_return = optimal_pool_data['Actual_Return'] / 100\n",
    "\n",
    "        liquidity_cost = 0\n",
    "        \n",
    "        #Only incur liquidity cost if there's a pool switch (gas fees paid twice) or it's the first period (gas fees paid once)\n",
    "        if period == 1 or max_sharpe_ratio_pool != current_pool[size]:\n",
    "            liquidity_cost = gas_fee if period == 1 else gas_fee * 2\n",
    "\n",
    "        #if expected return of switching larger than cost, then switch\n",
    "        if expected_return * current_size > liquidity_cost:\n",
    "            actual_return_noswitch = actual_return\n",
    "            current_pool[size] = max_sharpe_ratio_pool \n",
    "    \n",
    "        #if not larger then stay and take return of current pool\n",
    "        else:\n",
    "            actual_return_noswitch = df_final_RNN.loc[(df_final_RNN['start_date'] == date_mapping[period]) & (df_final_RNN['pool'] == current_pool[size]), 'return'].values[0] / 100\n",
    "            print(f\"Period {date_mapping[period]}: No liquidity provision due to high gas fees. Using actual return of {actual_return_noswitch*100}% for calculations.\")\n",
    "            \n",
    "        #The size of the position after the period is influenced by the return and the cost of switching the pool\n",
    "        #If pool is not switched then liquidity_cost = 0\n",
    "        new_position_size = current_size * (1 + actual_return_noswitch) - liquidity_cost \n",
    "        position_sizes[size].append(new_position_size)\n",
    "\n",
    "        current_size = new_position_size\n",
    "\n",
    "for size, positions in position_sizes.items():\n",
    "    print(f\"Final position size for initial size {size}: {positions[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting actual values, predicted values and the confidence interval around them\n",
    "for size in return_data:\n",
    "    df = pd.DataFrame({\n",
    "        'actual_values': return_data[size]['actual'],\n",
    "        'predicted_values': return_data[size]['predicted'],\n",
    "        'upper_bound': return_data[size]['upper'],\n",
    "        'lower_bound': return_data[size]['lower']\n",
    "    }, index=[date_mapping[i] for i in range(1, len(return_data[size]['actual']) + 1)])\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df['actual_values'], color='blue', label='Actual Values')\n",
    "    plt.plot(df['predicted_values'], color='red', label='Predicted Values')\n",
    "    plt.fill_between(df.index, df['lower_bound'], df['upper_bound'], color='gray', alpha=0.2, label='Confidence Interval')\n",
    "    plt.title(f'Returns for Initial Position', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Return (%)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131751fa",
   "metadata": {},
   "source": [
    "## Alternative baseline strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine the top pools by either size, volume or average return or lowest avg. vola\n",
    "def select_top_pools(df, date, n, metric, lookback_months=None, return_based=False, volatility_based=False):\n",
    "    #Consider whole timeframe for calculations\n",
    "    if lookback_months is None:\n",
    "        mask = (df['start_date'] < date)\n",
    "    #Only consider last n months\n",
    "    else:\n",
    "        lookback_date = date - pd.DateOffset(months=lookback_months)\n",
    "        mask = (df['start_date'] < date) & (df['start_date'] >= lookback_date)\n",
    "        \n",
    "    #For return mean of last n returns is taken and then top n pools\n",
    "    if return_based:\n",
    "        top_n_pools = df.loc[mask].groupby('pool')[metric].mean().nlargest(n).index.tolist()\n",
    "    #For vola mean of last n 30d_volas is taken and then top n pools with smallest\n",
    "    elif volatility_based:\n",
    "        top_n_pools = df.loc[mask].groupby('pool')[metric].mean().nsmallest(n).index.tolist()\n",
    "    #For other two metrics sum is taken and then top n pools\n",
    "    else:\n",
    "        top_n_pools = df.loc[mask].groupby('pool')[metric].sum().nlargest(n).index.tolist()\n",
    "\n",
    "    return top_n_pools\n",
    "\n",
    "#As before, three investment sizes\n",
    "initial_investments = [5000, 50000, 500000]\n",
    "\n",
    "#Defining the strategy pool (four metrics, two timehorizons, two num_pools -> 16 metrics)\n",
    "strategies = [\n",
    "    {\"lookback_months\": 6, \"num_pools\": 1, \"metric\": \"total_USD_value\"},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 5, \"metric\": \"total_USD_value\"},\n",
    "    {\"lookback_months\": None, \"num_pools\": 1, \"metric\": \"total_USD_value\"},\n",
    "    {\"lookback_months\": None, \"num_pools\": 5, \"metric\": \"total_USD_value\"},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 1, \"metric\": \"volume_usd\"},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 5, \"metric\": \"volume_usd\"},\n",
    "    {\"lookback_months\": None, \"num_pools\": 1, \"metric\": \"volume_usd\"},\n",
    "    {\"lookback_months\": None, \"num_pools\": 5, \"metric\": \"volume_usd\"},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 1, \"metric\": \"return\", \"return_based\": True},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 5, \"metric\": \"return\", \"return_based\": True},\n",
    "    {\"lookback_months\": None, \"num_pools\": 1, \"metric\": \"return\", \"return_based\": True},\n",
    "    {\"lookback_months\": None, \"num_pools\": 5, \"metric\": \"return\", \"return_based\": True},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 1, \"metric\": \"30D_volatility\", \"volatility_based\": True},\n",
    "    {\"lookback_months\": 6, \"num_pools\": 5, \"metric\": \"30D_volatility\", \"volatility_based\": True},\n",
    "    {\"lookback_months\": None, \"num_pools\": 1, \"metric\": \"30D_volatility\", \"volatility_based\": True},\n",
    "    {\"lookback_months\": None, \"num_pools\": 5, \"metric\": \"30D_volatility\", \"volatility_based\": True}\n",
    "]\n",
    "\n",
    "#Timeframe under consideration\n",
    "start_date = pd.to_datetime('2022-09-01')\n",
    "end_date = start_date + pd.DateOffset(months=5)\n",
    "\n",
    "#Assumed gas fee as before\n",
    "gas_fee = 7\n",
    "\n",
    "#Dictionary to store the amounts after each period for each investment amount and strategy (for plotting)\n",
    "amounts_after_each_period = {amount: {str(strategy): [] for strategy in strategies} for amount in initial_investments}\n",
    "\n",
    "#Looping through investment amounts\n",
    "for amount in initial_investments:\n",
    "    #Looping through strategies\n",
    "    for strategy in strategies:\n",
    "        current_amount = amount\n",
    "        first_time_investment = True\n",
    "        previous_pools = []\n",
    "        pool_investments = {pool: current_amount/len(previous_pools) if previous_pools else current_amount for pool in previous_pools}\n",
    "        #Looping through each month\n",
    "        for month in pd.date_range(start=start_date, end=end_date, freq='MS'):\n",
    "            #Selecting the top n pools\n",
    "            top_pools = select_top_pools(df_final_RNN, month, strategy['num_pools'], strategy['metric'], strategy['lookback_months'])\n",
    "            \n",
    "            #For the first investment period, the gas fees are only deducted once\n",
    "            #Also the amount invested in each pool is total amount / number of pools\n",
    "            if first_time_investment:\n",
    "                current_amount -= gas_fee * len(top_pools)\n",
    "                current_amount = current_amount / len(top_pools) if top_pools else current_amount\n",
    "                first_time_investment = False\n",
    "                \n",
    "            #For every subsequent period, it is checked whether pools have been switched\n",
    "            else:\n",
    "                switched_pools = set(previous_pools).symmetric_difference(set(top_pools))\n",
    "\n",
    "                #if pools have been switched then the amount currently invested in that pool is stored in current_amount and corrected with the gas fees for deopist + withdrawal\n",
    "                #Furthermore that entry is removed from the dict tracking the investments\n",
    "                for pool in switched_pools:\n",
    "                    if pool in pool_investments:\n",
    "                        current_amount = pool_investments[pool]\n",
    "                        current_amount -= gas_fee * 2\n",
    "                        del pool_investments[pool]\n",
    "            #The current amount is then attributed to the pool that has been switched into (which is currently in top_pools but not in pool_investments)            \n",
    "            for pool in top_pools:\n",
    "                if pool not in pool_investments: \n",
    "                    pool_investments[pool] = current_amount\n",
    "            \n",
    "            current_amount = 0\n",
    "            #For all pools that are in top_pools in a period their actual returns in that period are looked up and their returns are compounded\n",
    "            for pool in top_pools:\n",
    "                mask = (df_final_RNN['start_date'] == month) & (df_final_RNN['pool'] == pool)\n",
    "                pool_return = df_final_RNN.loc[mask, 'return'].values[0] / 100\n",
    "                pool_investments[pool] *= (1 + pool_return)\n",
    "\n",
    "            previous_pools = top_pools.copy()\n",
    "\n",
    "            #Calculates the total value of the investments at the end of all periods\n",
    "            current_amount = sum(pool_investments.values())\n",
    "            #Stores the investment amounts at the end of each period for each strategy\n",
    "            amounts_after_each_period[amount][str(strategy)].append(current_amount)\n",
    "\n",
    "        print(f\"Return after six months (Lookback: {strategy['lookback_months']}, Num Pools: {strategy['num_pools']}, Metric: {strategy['metric']}): {current_amount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caadf7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Renaming the strategies for plotting\n",
    "strategy_names = {\n",
    "    str({'lookback_months': 6, 'num_pools': 1, 'metric': 'total_USD_value'}): \"Pool size - Last six months- 1 pool\",\n",
    "    str({'lookback_months': 6, 'num_pools': 5, 'metric': 'total_USD_value'}): \"Pool size - Last six months - 5 pools\",\n",
    "    str({'lookback_months': None, 'num_pools': 1, 'metric': 'total_USD_value'}): \"Pool size - All time - 1 pool\",\n",
    "    str({'lookback_months': None, 'num_pools': 5, 'metric': 'total_USD_value'}): \"Pool size - All time - 5 pools\",\n",
    "    str({'lookback_months': 6, 'num_pools': 1, 'metric': 'volume_usd'}): \"Volume - Last six months- 1 pool\",\n",
    "    str({'lookback_months': 6, 'num_pools': 5, 'metric': 'volume_usd'}): \"Volume - Last six months - 5 pools\",\n",
    "    str({'lookback_months': None, 'num_pools': 1, 'metric': 'volume_usd'}): \"Volume - All time - 1 pool\",\n",
    "    str({'lookback_months': None, 'num_pools': 5, 'metric': 'volume_usd'}): \"Volume - All time - 5 pools\",\n",
    "    str({'lookback_months': 6, 'num_pools': 1, 'metric': 'return', 'return_based': True}): \"Avg. return - Last six months- 1 pool\",\n",
    "    str({'lookback_months': 6, 'num_pools': 5, 'metric': 'return', 'return_based': True}): \"Avg. return - Last six months - 5 pools\",\n",
    "    str({'lookback_months': None, 'num_pools': 1, 'metric': 'return', 'return_based': True}): \"Avg. return - All time - 1 pool\",\n",
    "    str({'lookback_months': None, 'num_pools': 5, 'metric': 'return', 'return_based': True}): \"Avg. return - All time - 5 pools\",\n",
    "    str({'lookback_months': 6, 'num_pools': 1, 'metric': '30D_volatility', 'volatility_based': True}): \"Avg. vola - Last six months- 1 pool\",\n",
    "    str({'lookback_months': 6, 'num_pools': 5, 'metric': '30D_volatility', 'volatility_based': True}): \"Avg. vola - Last six months - 5 pools\",\n",
    "    str({'lookback_months': None, 'num_pools': 1, 'metric': '30D_volatility', 'volatility_based': True}): \"Avg. vola - All time - 1 pool\",\n",
    "    str({'lookback_months': None, 'num_pools': 5, 'metric': '30D_volatility', 'volatility_based': True}): \"Avg. vola - All time - 5 pools\",\n",
    "}\n",
    "\n",
    "#Retrieving the final investment amounts for each strategy\n",
    "final_returns = {amount: {str(strategy): amounts[-1] if amounts else 0 for strategy, amounts in strategy_amounts.items()} for amount, strategy_amounts in amounts_after_each_period.items()}\n",
    "\n",
    "#Returns of the more profitable optimization (optimization by expected returns)\n",
    "custom_returns = [0.01, -0.001, -0.108, 0.164, 0.234, 0.035]\n",
    "\n",
    "custom_returns_2 = [-0.13, -0.001, -0.024, 0.164, 0.305, 0.056]\n",
    "\n",
    "def calculate_custom_amounts(amount, returns):\n",
    "    amounts = []\n",
    "    current_amount = amount\n",
    "    for return_ in returns:\n",
    "        current_amount *= (1 + return_)\n",
    "        amounts.append(current_amount)\n",
    "    return amounts\n",
    "\n",
    "def plot_strategies(strategies, title):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for strategy in strategies:\n",
    "        plt.plot(pd.date_range(start=start_date, end=end_date, freq='MS'), strategy_amounts[strategy], label=strategy_names.get(strategy, strategy))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Invested Amount\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "#Calculating custom returns\n",
    "custom_return_sets = [custom_returns, custom_returns_2]\n",
    "custom_return_names = [\"Custom: Predicted return\", \"Custom: Risk-adjusted return\"]\n",
    "\n",
    "for amount in initial_investments:\n",
    "    for returns, name in zip(custom_return_sets, custom_return_names):\n",
    "        amounts_after_each_period[amount][name] = calculate_custom_amounts(amount, returns)\n",
    "\n",
    "#Building final_returns\n",
    "final_returns = {}\n",
    "for amount, strategy_amounts in amounts_after_each_period.items():\n",
    "    final_returns[amount] = {}\n",
    "    for strategy, amounts in strategy_amounts.items():\n",
    "        final_returns[amount][str(strategy)] = amounts[-1] if amounts else 0\n",
    "\n",
    "#Plotting\n",
    "for amount, strategy_amounts in amounts_after_each_period.items():\n",
    "    #Plot all strategies\n",
    "    plot_strategies(strategy_amounts.keys(), \"All Strategies\")\n",
    "    \n",
    "    #Selecting top five strategies by return\n",
    "    top_strategies = sorted(final_returns[amount].items(), key=lambda item: item[1], reverse=True)[:5]\n",
    "\n",
    "    #Plotting top five strategies\n",
    "    plot_strategies([strategy for strategy, _ in top_strategies], f\"Top 5 Strategies\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
