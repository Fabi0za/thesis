{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "#To import parquet file with gas prices\n",
    "import pyarrow\n",
    "\n",
    "#For plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as offline\n",
    "import plotly.subplots as sp\n",
    "import plotly.io as pio\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "#For feature preprocessing and for tree-based models\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import Counter\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Bayesian optimization (one for torch one for sklearn)\n",
    "from skopt import BayesSearchCV\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "#For neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "import torch_optimizer as optim1\n",
    "import torch.optim as optim\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe with pool contract, pair and volume_usd until a certain timepoint\n",
    "Uniswap_pools_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/pool_Uniswap.csv\")\n",
    "#Dataframe with USD and swap prices\n",
    "Uniswap_prices_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/prices_Uniswap.csv\")\n",
    "#Dataframe with swap volume\n",
    "Uniswap_volume_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/volume_Uniswap.csv\")\n",
    "#Dataframe with liquidity pool tokens\n",
    "Uniswap_pool_tokens_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/liquidity_tokens_uniswap.csv\")\n",
    "\n",
    "#Same for Sushiswap\n",
    "Sushiswap_pools_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/pool_Sushiswap.csv\")\n",
    "Sushiswap_prices_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/prices_Sushiswap.csv\")\n",
    "Sushiswap_volume_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/volume_Sushiswap.csv\")\n",
    "Sushiswap_pool_tokens_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/liquidity_tokens_sushiswap.csv\")\n",
    "\n",
    "#Dataframe with gas fees\n",
    "gas_fees = pd.read_parquet('/Users/fabioza/Desktop/Master thesis/data/Uniswap_final/gas_ts_block.par', engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ed31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding a few Sushiswap pools that don't have enough data (FARM / WETH, KP3R / WETH, USDC / USDT)\n",
    "exclude_pools = [\"0x69b39b89f9274a16e8a19b78e5eb47a4d91dac9e\", \"0xaf988aff99d3d0cb870812c325c588d8d8cb7de8\", \"0xd86a120a06255df8d4e2248ab04d4267e23adfaa\"] \n",
    "\n",
    "Sushiswap_pools_df = Sushiswap_pools_df[~Sushiswap_pools_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_prices_df = Sushiswap_prices_df[~Sushiswap_prices_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_volume_df = Sushiswap_volume_df[~Sushiswap_volume_df[\"pool\"].isin(exclude_pools)]\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df[~Sushiswap_pool_tokens_df[\"pool_address\"].isin(exclude_pools)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting date columns to datetime\n",
    "Uniswap_pools_df['date'] = pd.to_datetime(Uniswap_pools_df['date'])\n",
    "Uniswap_prices_df['date'] = pd.to_datetime(Uniswap_prices_df['date'])\n",
    "Uniswap_volume_df['date'] = pd.to_datetime(Uniswap_volume_df['date'])   \n",
    "Uniswap_pool_tokens_df['date'] = pd.to_datetime(Uniswap_pool_tokens_df['date'])\n",
    "Sushiswap_pools_df['date'] = pd.to_datetime(Sushiswap_pools_df['date'])\n",
    "Sushiswap_prices_df['date'] = pd.to_datetime(Sushiswap_prices_df['date'])\n",
    "Sushiswap_volume_df['date'] = pd.to_datetime(Sushiswap_volume_df['date'])   \n",
    "Sushiswap_pool_tokens_df['date'] = pd.to_datetime(Sushiswap_pool_tokens_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Uniswap_pool_tokens_df and Sushiswap_pool_tokens_df\n",
    "#Remove columns\n",
    "Uniswap_pool_tokens_df = Uniswap_pool_tokens_df.drop(['Unnamed: 0'], axis=1)\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df.drop(['Unnamed: 0'], axis=1)\n",
    "#Rename columns\n",
    "Uniswap_pool_tokens_df = Uniswap_pool_tokens_df.rename(columns={\"totalSupply\": \"pool_token_amount\", 'pool_address': 'pool'})\n",
    "Sushiswap_pool_tokens_df = Sushiswap_pool_tokens_df.rename(columns={\"totalSupply\": \"pool_token_amount\", 'pool_address': 'pool'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db219fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Sushiswap_volume_df: Rename columns\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.rename(columns={\"token_it\": \"token_in\", '_col3': 'max_token_pair'})\n",
    "\n",
    "#Only take last observation for a day (df currently contains hourly observations)\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.sort_values('hour').groupby(['pool', 'date'], as_index=False).last()\n",
    "Sushiswap_volume_df = Sushiswap_volume_df.drop('hour', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating dfs to get one df\n",
    "complete_volume_df = pd.concat([Uniswap_volume_df, Sushiswap_volume_df])\n",
    "complete_pool_tokens_df = pd.concat([Uniswap_pool_tokens_df, Sushiswap_pool_tokens_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average daily gas fees\n",
    "gas_fees['ts'] = pd.to_datetime(gas_fees['ts'])\n",
    "gas_fees = gas_fees.set_index('ts')\n",
    "average_daily_gas_fees = gas_fees.resample('D')['avg_gas'].mean()\n",
    "average_daily_gas_fees = average_daily_gas_fees.reset_index()\n",
    "average_daily_gas_fees = average_daily_gas_fees.rename(columns={\"ts\": \"date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a mapping that allows correlating smart contract addresses to liq. pairs\n",
    "pool_mapping_Uniswap = {\n",
    "    '0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc': ('USDC', 'WETH'),\n",
    "    '0x0d4a11d5eeaac28ec3f61d100daf4d40471f1852': ('WETH', 'USDT'),\n",
    "    '0xa478c2975ab1ea89e8196811f51a7b7ade33eb11': ('DAI', 'WETH'),\n",
    "    '0xbb2b8038a1640196fbe3e38816f3e67cba72d940': ('WBTC', 'WETH'),\n",
    "    '0x2fdbadf3c4d5a8666bc06645b8358ab803996e28': ('YFI', 'WETH'),\n",
    "    '0xd3d2e2692501a5c9ca623199d38826e513033a17': ('UNI', 'WETH'),\n",
    "    '0xa2107fa5b38d9bbd2c461d6edf11b11a50f6b974': ('LINK', 'WETH'),\n",
    "    '0xc5be99a02c6857f9eac67bbce58df5572498f40c': ('WETH', 'AMPL'),\n",
    "    '0x3041cbd36888becc7bbcbc0045e3b1f144466f5f': ('USDC', 'USDT'),\n",
    "    '0xdfc14d2af169b0d36c4eff567ada9b2e0cae044f': ('AAVE', 'WETH'),\n",
    "    '0xab3f9bf1d81ddb224a2014e98b238638824bcf20': ('LEND', 'WETH'),\n",
    "    '0xc2adda861f89bbb333c90c492cb837741916a225': ('MKR', 'WETH'),\n",
    "    '0x43ae24960e5534731fc831386c07755a2dc33d47': ('SNX', 'WETH'),\n",
    "    '0x3da1313ae46132a397d90d95b1424a9a7e3e0fce': ('WETH', 'CRV'),\n",
    "    '0x87febfb3ac5791034fd5ef1a615e9d9627c2665d': ('KP3R', 'WETH'),\n",
    "    '0x32ce7e48debdccbfe0cd037cc89526e4382cb81b': ('CORE', 'WETH'),\n",
    "    '0xcffdded873554f362ac02f8fb1f02e5ada10516f': ('COMP', 'WETH'),\n",
    "    '0xdc98556ce24f007a5ef6dc1ce96322d65832a819': ('PICKLE', 'WETH'),\n",
    "    '0x56feaccb7f750b997b36a68625c7c596f0b41a58': ('FARM', 'WETH'),\n",
    "    '0xae461ca67b15dc8dc81ce7615e0320da1a9ab8d5': ('DAI', 'USDC'),\n",
    "    '0xddf9b7a31b32ebaf5c064c80900046c9e5b7c65f': ('CREAM', 'WETH'),\n",
    "    '0xb20bd5d04be54f870d5c0d3ca85d82b34b836405': ('DAI', 'USDT'),\n",
    "    '0x88d97d199b9ed37c29d846d00d443de980832a22': ('UMA', 'WETH'),\n",
    "}\n",
    "\n",
    "pool_mapping_Sushiswap = {\n",
    "    '0x397ff1542f962076d0bfe58ea045ffa2d347aca0': ('USDC', 'WETH'),\n",
    "    '0x06da0fd433c1a5d7a4faa01111c044910a184553': ('WETH', 'USDT'),\n",
    "    '0xc3d03e4f041fd4cd388c549ee2a29a9e5075882f': ('DAI', 'WETH'),\n",
    "    '0xceff51756c56ceffca006cd410b03ffc46dd3a58': ('WBTC', 'WETH'),\n",
    "    '0x088ee5007c98a9677165d78dd2109ae4a3d04d0c': ('YFI', 'WETH'),\n",
    "    '0xdafd66636e2561b0284edde37e42d192f2844d40': ('UNI', 'WETH'),\n",
    "    '0xc40d16476380e4037e6b1a2594caf6a6cc8da967': ('LINK', 'WETH'),\n",
    "    '0xcb2286d9471cc185281c4f763d34a962ed212962': ('WETH', 'AMPL'),\n",
    "    '0xd86a120a06255df8d4e2248ab04d4267e23adfaa': ('USDC', 'USDT'),\n",
    "    '0xd75ea151a61d06868e31f8988d28dfe5e9df57b4': ('AAVE', 'WETH'),\n",
    "    '0xba13afecda9beb75de5c56bbaf696b880a5a50dd': ('MKR', 'WETH'),\n",
    "    '0xa1d7b2d891e3a1f9ef4bbc5be20630c2feb1c470': ('SNX', 'WETH'),\n",
    "    '0x58dc5a51fe44589beb22e8ce67720b5bc5378009': ('WETH', 'CRV'),\n",
    "    '0xaf988aff99d3d0cb870812c325c588d8d8cb7de8': ('KP3R', 'WETH'),\n",
    "    '0x68c6d02d44e16f1c20088731ab032f849100d70f': ('CORE', 'WETH'),\n",
    "    '0x31503dcb60119a812fee820bb7042752019f2355': ('COMP', 'WETH'),\n",
    "    '0x269db91fc3c7fcc275c2e6f22e5552504512811c': ('PICKLE', 'WETH'),\n",
    "    '0xf169cea51eb51774cf107c88309717dda20be167': ('CREAM', 'WETH'),\n",
    "    '0x001b6450083e531a5a7bf310bd2c1af4247e23d4': ('UMA', 'WETH'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matching pools to smart contract addresses\n",
    "Uniswap_pools_df['symbol_1'] = Uniswap_pools_df['pool'].map(lambda x: pool_mapping_Uniswap.get(x, (None, None))[0])\n",
    "Uniswap_pools_df['symbol_2'] = Uniswap_pools_df['pool'].map(lambda x: pool_mapping_Uniswap.get(x, (None, None))[1])\n",
    "Sushiswap_pools_df['symbol_1'] = Sushiswap_pools_df['pool'].map(lambda x: pool_mapping_Sushiswap.get(x, (None, None))[0])\n",
    "Sushiswap_pools_df['symbol_2'] = Sushiswap_pools_df['pool'].map(lambda x: pool_mapping_Sushiswap.get(x, (None, None))[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting the token amounts for Uniswap pools\n",
    "pool_list_1 = [\"0x2fdbadf3c4d5a8666bc06645b8358ab803996e28\", \"0x32ce7e48debdccbfe0cd037cc89526e4382cb81b\", \"0x3da1313ae46132a397d90d95b1424a9a7e3e0fce\", \"0x43ae24960e5534731fc831386c07755a2dc33d47\", \"0x56feaccb7f750b997b36a68625c7c596f0b41a58\", \"0x87febfb3ac5791034fd5ef1a615e9d9627c2665d\", \"0x88d97d199b9ed37c29d846d00d443de980832a22\", \"0xa2107fa5b38d9bbd2c461d6edf11b11a50f6b974\", \"0xa478c2975ab1ea89e8196811f51a7b7ade33eb11\", \"0xab3f9bf1d81ddb224a2014e98b238638824bcf20\", \"0xc2adda861f89bbb333c90c492cb837741916a225\", \"0xcffdded873554f362ac02f8fb1f02e5ada10516f\", \"0xd3d2e2692501a5c9ca623199d38826e513033a17\", \"0xdc98556ce24f007a5ef6dc1ce96322d65832a819\", \"0xddf9b7a31b32ebaf5c064c80900046c9e5b7c65f\", \"0xdfc14d2af169b0d36c4eff567ada9b2e0cae044f\"]\n",
    "\n",
    "for pool in pool_list_1:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "\n",
    "pool_list_2 = [\"0x3041cbd36888becc7bbcbc0045e3b1f144466f5f\"]    \n",
    "    \n",
    "for pool in pool_list_2:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_3 = [\"0xae461ca67b15dc8dc81ce7615e0320da1a9ab8d5\", \"0xb20bd5d04be54f870d5c0d3ca85d82b34b836405\", \"0x0d4a11d5eeaac28ec3f61d100daf4d40471f1852\"]    \n",
    "    \n",
    "for pool in pool_list_3:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_4 = [\"0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc\"]    \n",
    "    \n",
    "for pool in pool_list_4:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_5 = [\"0xbb2b8038a1640196fbe3e38816f3e67cba72d940\"]    \n",
    "    \n",
    "for pool in pool_list_5:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e8\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_6 = [\"0xc5be99a02c6857f9eac67bbce58df5572498f40c\"]    \n",
    "    \n",
    "for pool in pool_list_6:\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] = Uniswap_pools_df.loc[Uniswap_pools_df[\"pool\"] == pool, 'y'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting the token amounts for Sushiswap pools\n",
    "pool_list_1 = [\"0x088ee5007c98a9677165d78dd2109ae4a3d04d0c\", \"0x68c6d02d44e16f1c20088731ab032f849100d70f\", \"0x58dc5a51fe44589beb22e8ce67720b5bc5378009\", \"0xa1d7b2d891e3a1f9ef4bbc5be20630c2feb1c470\", \"0xaf988aff99d3d0cb870812c325c588d8d8cb7de8\", \"0x001b6450083e531a5a7bf310bd2c1af4247e23d4\", \"0xc40d16476380e4037e6b1a2594caf6a6cc8da967\", \"0xc3d03e4f041fd4cd388c549ee2a29a9e5075882f\", \"0xba13afecda9beb75de5c56bbaf696b880a5a50dd\", \"0x31503dcb60119a812fee820bb7042752019f2355\", \"0xdafd66636e2561b0284edde37e42d192f2844d40\", \"0x269db91fc3c7fcc275c2e6f22e5552504512811c\", \"0xf169cea51eb51774cf107c88309717dda20be167\", \"0xd75ea151a61d06868e31f8988d28dfe5e9df57b4\"]\n",
    "\n",
    "for pool in pool_list_1:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "\n",
    "pool_list_2 = [\"0xd86a120a06255df8d4e2248ab04d4267e23adfaa\"]    \n",
    "    \n",
    "for pool in pool_list_2:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_3 = [\"0x06da0fd433c1a5d7a4faa01111c044910a184553\"]    \n",
    "    \n",
    "for pool in pool_list_3:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e6\n",
    "    \n",
    "pool_list_4 = [\"0x397ff1542f962076d0bfe58ea045ffa2d347aca0\"]    \n",
    "    \n",
    "for pool in pool_list_4:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e6\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_5 = [\"0xceff51756c56ceffca006cd410b03ffc46dd3a58\"]    \n",
    "    \n",
    "for pool in pool_list_5:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e8\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e18\n",
    "    \n",
    "pool_list_6 = [\"0xcb2286d9471cc185281c4f763d34a962ed212962\"]    \n",
    "    \n",
    "for pool in pool_list_6:\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'x'] / 1e18\n",
    "    Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] = Sushiswap_pools_df.loc[Sushiswap_pools_df[\"pool\"] == pool, 'y'] / 1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33879c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a flag indicating which DEX the data is from and concatenating the two dfs\n",
    "Uniswap_pools_df['Uniswap'] = '1'\n",
    "Sushiswap_pools_df['Sushiswap'] = '1'\n",
    "complete_pools_df = pd.concat([Uniswap_pools_df, Sushiswap_pools_df])\n",
    "complete_pools_df = complete_pools_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43387bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating swap rates between tokens based on amount of tokens in pool\n",
    "complete_pools_df[\"avg_swap_price\"] = complete_pools_df[\"y\"] / complete_pools_df[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating USD swap rates and rearranging direction of token swap rates\n",
    "Sushiswap_prices_df = Sushiswap_prices_df.rename(columns={\"median_usd_price_token_sold\": \"avg_usd_price_token_sold\", \"median_usd_price_token_bought\": \"avg_usd_price_token_bought\", \"median_swap_price\": \"avg_swap_price\"})\n",
    "\n",
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "def filter_and_append(df, filtered_df, pool_mapping):\n",
    "    for pool_address, (token_a, token_b) in pool_mapping.items():\n",
    "        filtered_rows = df[(df['pool'] == pool_address) &\n",
    "                           ((df['token_bought_symbol'] == token_a) & (df['token_sold_symbol'] == token_b) |\n",
    "                            (df['token_bought_symbol'] == token_b) & (df['token_sold_symbol'] == token_a))]\n",
    "\n",
    "        grouped_rows = filtered_rows.groupby('date')\n",
    "\n",
    "        for date, group in grouped_rows:\n",
    "            #Check if both directions are present (account for the fact that trades can happen in both directions)\n",
    "            if len(group) == 2:\n",
    "                #Keep the preferred direction (based on what was defined in dict above)\n",
    "                preferred_row = group[(group['token_bought_symbol'] == token_a) & (group['token_sold_symbol'] == token_b)]\n",
    "            else:\n",
    "                preferred_row = group\n",
    "\n",
    "                #Check if direction needs to be switched\n",
    "                if preferred_row.iloc[0]['token_bought_symbol'] != token_a:\n",
    "                    temp_token_bought_symbol = preferred_row.loc[:, 'token_bought_symbol'].copy()\n",
    "                    preferred_row.loc[:, 'token_bought_symbol'] = preferred_row.loc[:, 'token_sold_symbol']\n",
    "                    preferred_row.loc[:, 'token_sold_symbol'] = temp_token_bought_symbol\n",
    "\n",
    "                    #Switch avg_usd_price_token_sold and avg_usd_price_token_bought\n",
    "                    temp_avg_usd_price_token_sold = preferred_row.loc[:, 'avg_usd_price_token_sold'].copy()\n",
    "                    preferred_row.loc[:, 'avg_usd_price_token_sold'] = preferred_row.loc[:, 'avg_usd_price_token_bought']\n",
    "                    preferred_row.loc[:, 'avg_usd_price_token_bought'] = temp_avg_usd_price_token_sold\n",
    "\n",
    "                    #Calculate avg_swap_price in opposite direction\n",
    "                    preferred_row.loc[:, 'avg_swap_price'] = 1 / preferred_row.loc[:, 'avg_swap_price']\n",
    "            filtered_df = pd.concat([filtered_df, preferred_row], ignore_index=True)\n",
    "    return filtered_df\n",
    "\n",
    "filtered_df = filter_and_append(Uniswap_prices_df, filtered_df, pool_mapping_Uniswap)\n",
    "filtered_df = filter_and_append(Sushiswap_prices_df, filtered_df, pool_mapping_Sushiswap)\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df[['avg_usd_price_token_sold', 'avg_usd_price_token_bought', 'avg_swap_price']] = filtered_df[['avg_usd_price_token_sold', 'avg_usd_price_token_bought', 'avg_swap_price']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace extreme outliers with the last non-outlier price: Using a rolling-window approach given the outlier nature of the data\n",
    "#Window size is number of days taken into account and threshold determines how many standard deviations a value has to deviate to be considered an outlier\n",
    "def remove_rolling_outliers(df, pool_name, window_size=30, threshold=1.5):\n",
    "    pool_df = df[df['pool'] == pool_name].sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    #Rolling mean and stdev calculation\n",
    "    pool_df['rolling_mean'] = pool_df['avg_swap_price'].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    pool_df['rolling_std'] = pool_df['avg_swap_price'].rolling(window=window_size, center=True, min_periods=1).std()\n",
    "\n",
    "    #Identification of outliers\n",
    "    outliers = (pool_df['avg_swap_price'] < (pool_df['rolling_mean'] - threshold * pool_df['rolling_std'])) | \\\n",
    "               (pool_df['avg_swap_price'] > (pool_df['rolling_mean'] + threshold * pool_df['rolling_std']))\n",
    "\n",
    "    #Iterating through the rows and updating values if current row is outlier\n",
    "    for index, row in pool_df.loc[outliers].iterrows():\n",
    "        outlier_date = row['date']\n",
    "\n",
    "        #Updating outlier value with last non-outlier value\n",
    "        last_valid_day = pool_df.loc[(pool_df['date'] < outlier_date) & ~outliers].sort_values('date', ascending=False).head(1)\n",
    "        if not last_valid_day.empty:\n",
    "            last_values = last_valid_day[['avg_swap_price', 'avg_usd_price_token_sold', 'avg_usd_price_token_bought']].values[0]\n",
    "            pool_df.loc[index, ['avg_swap_price', 'avg_usd_price_token_sold', 'avg_usd_price_token_bought']] = last_values\n",
    "\n",
    "    return pool_df.drop(columns=['rolling_mean', 'rolling_std'])\n",
    "\n",
    "#Removing the outliers\n",
    "pool_names = filtered_df['pool'].unique()\n",
    "processed_data = pd.concat([remove_rolling_outliers(filtered_df, pool) for pool in pool_names]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing beginning of month prices to check whether they look fine\n",
    "#Creating a new column to store DEX name\n",
    "processed_data[\"platform\"] = np.nan\n",
    "for pool in processed_data[\"pool\"].unique():\n",
    "    if pool in pool_mapping_Uniswap:\n",
    "        processed_data.loc[processed_data[\"pool\"] == pool, \"platform\"] = \"Uniswap\"\n",
    "    elif pool in pool_mapping_Sushiswap:\n",
    "        processed_data.loc[processed_data[\"pool\"] == pool, \"platform\"] = \"Sushiswap\"\n",
    "\n",
    "processed_data['date'] = pd.to_datetime(processed_data['date'])\n",
    "processed_data.set_index('date', inplace=True)\n",
    "\n",
    "#Separating Uniswap and Sushiswap data\n",
    "processed_data_Uniswap = processed_data[processed_data[\"platform\"] == \"Uniswap\"]\n",
    "processed_data_Sushiswap = processed_data[processed_data[\"platform\"] == \"Sushiswap\"]\n",
    "\n",
    "#Function to process data\n",
    "def process_data(processed_data):\n",
    "    grouped_df = processed_data.groupby(\"pool\")\n",
    "    bom_df = pd.DataFrame()\n",
    "\n",
    "    #Resampling the data to get beginning of month data\n",
    "    for pool, group_df in grouped_df:\n",
    "        \n",
    "        pool_bom_df = group_df.resample(\"MS\").last()\n",
    "        bom_df = pd.concat([bom_df, pool_bom_df], ignore_index=False)\n",
    "    \n",
    "    #Interpolating nan values\n",
    "    bom_df.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "\n",
    "    #Storing beginning of month prices for each unique token\n",
    "    unique_tokens = pd.concat([bom_df[\"token_bought_symbol\"], bom_df[\"token_sold_symbol\"]]).unique()\n",
    "    bom_usd_prices = pd.DataFrame(index=bom_df.index.unique(), columns=unique_tokens)\n",
    "    for token in unique_tokens:\n",
    "        token_rows = bom_df[(bom_df[\"token_bought_symbol\"] == token) | (bom_df[\"token_sold_symbol\"] == token)]\n",
    "        token_rows = token_rows.loc[~token_rows.index.duplicated(keep='first')]\n",
    "        bom_usd_prices[token] = token_rows.apply(lambda row: row[\"avg_usd_price_token_bought\"] if row[\"token_bought_symbol\"] == token else row[\"avg_usd_price_token_sold\"], axis=1)\n",
    "    \n",
    "    #Interpolating usd prices that are still missing\n",
    "    bom_usd_prices.interpolate(method=\"linear\", inplace=True, limit_direction=\"both\")\n",
    "    return bom_usd_prices\n",
    "\n",
    "#Applying function to Uniswap and Sushiswap data separately\n",
    "bom_usd_prices_Uniswap = process_data(processed_data_Uniswap)\n",
    "bom_usd_prices_Sushiswap = process_data(processed_data_Sushiswap)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "bom_usd_prices_Uniswap.plot(ax=ax[0])\n",
    "ax[0].set_ylabel(\"USD Price\")\n",
    "ax[0].set_title(\"USD Price at the Beginning of Each Month for Unique Tokens on Uniswap\")\n",
    "ax[0].legend(title=\"Tokens\")\n",
    "bom_usd_prices_Sushiswap.plot(ax=ax[1])\n",
    "ax[1].set_ylabel(\"USD Price\")\n",
    "ax[1].set_xlabel(\"Date\")\n",
    "ax[1].set_title(\"USD Price at the Beginning of Each Month for Unique Tokens on Sushiswap\")\n",
    "ax[1].legend(title=\"Tokens\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f31df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the dataframes\n",
    "complete_pools_df = pd.merge(complete_pools_df, complete_volume_df, on=['pool', 'date'])\n",
    "\n",
    "#Removing unnecessary columns\n",
    "complete_pools_df = complete_pools_df.drop(columns = [\"max_token_pair\", \"token_in\", \"token_out\"])\n",
    "\n",
    "#Calculating order flow imbalance\n",
    "complete_pools_df[\"imbalance\"] = complete_pools_df[\"volume_in\"] - complete_pools_df[\"volume_out\"]\n",
    "\n",
    "#Adding gas fees to dataframe\n",
    "complete_pools_df = pd.merge(complete_pools_df, average_daily_gas_fees, on=['date'])\n",
    "\n",
    "#Adding liquidity tokens to dataframe\n",
    "complete_pools_df = pd.merge(complete_pools_df, complete_pool_tokens_df, on=['pool', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the total pool value (=pool size) for each day\n",
    "def calculate_total_pool_value(prices_df, pools_df):\n",
    "    prices_df_reversed = prices_df.copy()\n",
    "    prices_df_reversed.rename(columns={\n",
    "        'token_bought_symbol': 'token_sold_symbol', \n",
    "        'token_sold_symbol': 'token_bought_symbol',\n",
    "        'avg_usd_price_token_bought': 'avg_usd_price_token_sold',\n",
    "        'avg_usd_price_token_sold': 'avg_usd_price_token_bought'\n",
    "    }, inplace=True)\n",
    "\n",
    "    prices_df_combined = pd.concat([prices_df, prices_df_reversed])\n",
    "\n",
    "    merged_df = pd.merge(pools_df, prices_df_combined, \n",
    "                         left_on=['date', 'symbol_1', 'symbol_2'], \n",
    "                         right_on=['date', 'token_bought_symbol', 'token_sold_symbol'], \n",
    "                         how='left')\n",
    "\n",
    "    merged_df['USD_value_symbol_1'] = merged_df['x'] * merged_df['avg_usd_price_token_bought']\n",
    "    merged_df['USD_value_symbol_2'] = merged_df['y'] * merged_df['avg_usd_price_token_sold']\n",
    "    merged_df['total_USD_value'] = merged_df['USD_value_symbol_1'] + merged_df['USD_value_symbol_2']\n",
    "    merged_df = merged_df.rename({'pool_x': 'pool'}, axis=1)\n",
    "    merged_df = merged_df[['pool', 'date', 'total_USD_value']]\n",
    "    merged_df = merged_df.drop_duplicates(subset=['pool', 'date'])\n",
    "    return merged_df\n",
    "\n",
    "#Calculate for Uniswap\n",
    "Uniswap_merged_df = calculate_total_pool_value(Uniswap_prices_df, Uniswap_pools_df)\n",
    "\n",
    "#Calculate for Sushiswap\n",
    "Sushiswap_merged_df = calculate_total_pool_value(Sushiswap_prices_df, Sushiswap_pools_df)\n",
    "\n",
    "#Concatenate both dataframes\n",
    "final_df = pd.concat([Uniswap_merged_df, Sushiswap_merged_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging pool size with the complete dataframe\n",
    "complete_pools_df = pd.merge(final_df, complete_pools_df, on=['pool', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a categorical variable that declares the pool type\n",
    "#Clearly specifying exotic and stable pairs and adding rest to normal pairs\n",
    "pair_categories = {\n",
    "    'WETH-AMPL': 'Exotic',\n",
    "    'USDC-USDT': 'Stable',\n",
    "    'DAI-USDC': 'Stable',\n",
    "    'DAI-USDT': 'Stable',\n",
    "    'PICKLE-WETH': 'Exotic',\n",
    "    'FARM-WETH': 'Exotic',\n",
    "    'CREAM-WETH': 'Exotic',\n",
    "     'CORE-WETH': 'Exotic'\n",
    "}\n",
    "\n",
    "def assign_category(row):\n",
    "    pair = row['symbol_1'] + '-' + row['symbol_2']\n",
    "    return pair_categories.get(pair, 'Normal')\n",
    "\n",
    "complete_pools_df['pool_category'] = complete_pools_df.apply(assign_category, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting DEX flags to dtype int\n",
    "complete_pools_df['Uniswap'] = complete_pools_df['Uniswap'].astype('int')\n",
    "complete_pools_df['Sushiswap'] = complete_pools_df['Sushiswap'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating profits of liquidity provision on both dex across cross-section of pairs\n",
    "bom_usd_prices_dataframes = {\n",
    "    \"Uniswap\": bom_usd_prices_Uniswap, \n",
    "    \"Sushiswap\": bom_usd_prices_Sushiswap\n",
    "}\n",
    "\n",
    "complete_pools_Uniswap_df = complete_pools_df[complete_pools_df['Uniswap'] == 1]\n",
    "complete_pools_Sushiswap_df = complete_pools_df[complete_pools_df['Sushiswap'] == 1]\n",
    "\n",
    "complete_pools_Uniswap_df.set_index('date', inplace=True)\n",
    "complete_pools_Sushiswap_df.set_index('date', inplace=True)\n",
    "complete_pools_df = complete_pools_df.set_index(\"date\")\n",
    "\n",
    "#How much of the pools total liquidity to be invested\n",
    "investment_percentages = [0.01]  \n",
    "\n",
    "#timeframe of observation (equals ten three month periods)\n",
    "start_date = pd.to_datetime('2020-10-01')\n",
    "end_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "#calculating profit for each DEX\n",
    "profit_results = []\n",
    "\n",
    "for platform, complete_pools_df in [(\"Uniswap\", complete_pools_Uniswap_df), (\"Sushiswap\", complete_pools_Sushiswap_df)]:\n",
    "    bom_usd_prices = bom_usd_prices_dataframes[platform]\n",
    "\n",
    "    start_date = pd.to_datetime('2020-10-01')\n",
    "    end_date = pd.to_datetime('2023-04-01')\n",
    "\n",
    "    while start_date < end_date:\n",
    "        next_date = start_date + pd.DateOffset(months=3)\n",
    "\n",
    "        for pool in complete_pools_df['pool'].unique():\n",
    "            pool_data = complete_pools_df[complete_pools_df['pool'] == pool]\n",
    "\n",
    "            pool_data_monthly = pool_data.resample('MS').first()\n",
    "\n",
    "            token_bought_symbol = pool_data['symbol_1'].iloc[0]\n",
    "            token_sold_symbol = pool_data['symbol_2'].iloc[0]\n",
    "\n",
    "            bom_prices_token_bought = bom_usd_prices[token_bought_symbol]\n",
    "            bom_prices_token_sold = bom_usd_prices[token_sold_symbol]\n",
    "\n",
    "            #Calculating USD pool value of each token in a pool at timepoint t_1 and t_2\n",
    "            pool_value_t1_token_bought = pool_data_monthly.loc[start_date, 'x'] * bom_prices_token_bought.loc[start_date]\n",
    "            pool_value_t1_token_sold = pool_data_monthly.loc[start_date, 'y'] * bom_prices_token_sold.loc[start_date]\n",
    "            pool_value_t2_token_bought = pool_data_monthly.loc[next_date, 'x'] * bom_prices_token_bought.loc[next_date]\n",
    "            pool_value_t2_token_sold = pool_data_monthly.loc[next_date, 'y'] * bom_prices_token_sold.loc[next_date]\n",
    "\n",
    "            #Calculating the total pool value by summing both\n",
    "            total_pool_value_t1 = pool_value_t1_token_bought + pool_value_t1_token_sold\n",
    "            total_pool_value_t2 = pool_value_t2_token_bought + pool_value_t2_token_sold\n",
    "\n",
    "            #Getting amount of pool tokens at t_1 and t_2\n",
    "            lp_tokens_t1 = pool_data_monthly.loc[start_date, 'pool_token_amount']\n",
    "            lp_tokens_t2 = pool_data_monthly.loc[next_date, 'pool_token_amount']\n",
    "            \n",
    "            for investment_percentage in investment_percentages:\n",
    "                #Calculating how much is invested in terms of USD at t_1\n",
    "                invest_t1 = investment_percentage * total_pool_value_t1\n",
    "                #Calculating the amount of liquidity tokens a liq. providers gets in t_1 based on the investment amount\n",
    "                amount0 = investment_percentage * pool_data_monthly.loc[start_date, 'x']\n",
    "                amount1 = investment_percentage * pool_data_monthly.loc[start_date, 'y']\n",
    "                _totalSupply = pool_data_monthly.loc[start_date, 'pool_token_amount']\n",
    "                _reserve0 = pool_data_monthly.loc[start_date, 'x']\n",
    "                _reserve1 = pool_data_monthly.loc[start_date, 'y']\n",
    "                liquidity_t1 = min(amount0 * _totalSupply / _reserve0, amount1 * _totalSupply / _reserve1)\n",
    "\n",
    "                #Calculating how much (both absolute and relative) of the liquidity tokens the same LP owns in t_2\n",
    "                lp_tokens_owned = liquidity_t1\n",
    "                percentage_t2 = lp_tokens_owned / lp_tokens_t2\n",
    "                #Based on this calculating the USD value of his liq. position\n",
    "                invest_t2 = percentage_t2 * total_pool_value_t2\n",
    "                #Calculating the return of just holding the tokens\n",
    "                hold_t1 = invest_t1\n",
    "                hold_t2 = amount0 * bom_prices_token_bought.loc[next_date] / bom_prices_token_bought.loc[start_date] * bom_prices_token_bought.loc[next_date]  + amount1 * bom_prices_token_sold.loc[next_date] / bom_prices_token_sold.loc[start_date] * bom_prices_token_sold.loc[next_date] \n",
    "                \n",
    "                #Calculating the return of providing liquidity vs holding the token\n",
    "                return_value = 100 * ((invest_t2 / hold_t1) - (invest_t1 / hold_t1)) / (invest_t1 / hold_t1)\n",
    "\n",
    "                #Appending profits and other variables to dict\n",
    "                profit_results.append({\n",
    "                    'platform': platform,\n",
    "                    'pool': pool,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': next_date,\n",
    "                    'return': return_value\n",
    "                })\n",
    "\n",
    "        start_date = next_date\n",
    "\n",
    "#Converting dict to dataframe\n",
    "profit_df = pd.DataFrame(profit_results)\n",
    "print(profit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating the two pool dfs together again, as they get split in previous codeblock\n",
    "complete_pools_df = pd.concat([complete_pools_Uniswap_df, complete_pools_Sushiswap_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding token symbols to contract addresses (for identification)\n",
    "def get_token_symbols(pool_address):\n",
    "    return pool_mapping_Uniswap.get(pool_address, ('Unknown', 'Unknown'))\n",
    "profit_df['symbol_1'], profit_df['symbol_2'] = zip(*profit_df['pool'].map(get_token_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558aca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting profits for both pools\n",
    "#Splitting data by DEX\n",
    "profit_df_Uniswap = profit_df[profit_df['platform'] == 'Uniswap']\n",
    "profit_df_Sushiswap = profit_df[profit_df['platform'] == 'Sushiswap']\n",
    "\n",
    "#Uniswap plot\n",
    "traces_Uniswap = []\n",
    "for pool in profit_df_Uniswap['pool'].unique():\n",
    "    pool_data = profit_df_Uniswap[profit_df_Uniswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        name=f'{pool} Pool'\n",
    "    )\n",
    "    traces_Uniswap.append(trace)\n",
    "\n",
    "layout_Uniswap = go.Layout(\n",
    "    title='3-months Uniswap Pool profit over time',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Profit (%)')\n",
    ")\n",
    "\n",
    "fig_Uniswap = go.Figure(data=traces_Uniswap, layout=layout_Uniswap)\n",
    "\n",
    "#Sushiswap plot\n",
    "traces_Sushiswap = []\n",
    "for pool in profit_df_Sushiswap['pool'].unique():\n",
    "    pool_data = profit_df_Sushiswap[profit_df_Sushiswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        name=f'{pool} Pool'\n",
    "    )\n",
    "    traces_Sushiswap.append(trace)\n",
    "\n",
    "layout_Sushiswap = go.Layout(\n",
    "    title='3-months Sushiswap Pool profit over time',\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='Profit (%)')\n",
    ")\n",
    "\n",
    "fig_Sushiswap = go.Figure(data=traces_Sushiswap, layout=layout_Sushiswap)\n",
    "\n",
    "offline.init_notebook_mode(connected=True)\n",
    "offline.plot(fig_Uniswap, filename='Uniswap_profit_plot.html', auto_open=False)\n",
    "offline.plot(fig_Sushiswap, filename='Sushiswap_profit_plot.html', auto_open=False)\n",
    "offline.iplot(fig_Uniswap)\n",
    "offline.iplot(fig_Sushiswap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same but plots next to each other\n",
    "traces_Uniswap = []\n",
    "for pool in profit_df_Uniswap['pool'].unique():\n",
    "    pool_data = profit_df_Uniswap[profit_df_Uniswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    traces_Uniswap.append(trace)\n",
    "\n",
    "traces_Sushiswap = []\n",
    "for pool in profit_df_Sushiswap['pool'].unique():\n",
    "    pool_data = profit_df_Sushiswap[profit_df_Sushiswap['pool'] == pool]\n",
    "    trace = go.Scatter(\n",
    "        x=pool_data['start_date'],\n",
    "        y=pool_data['return'],\n",
    "        mode='lines',\n",
    "        showlegend=False\n",
    "    )\n",
    "    traces_Sushiswap.append(trace)\n",
    "\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=('Uniswap', 'Sushiswap'))\n",
    "\n",
    "for trace in traces_Uniswap:\n",
    "    fig.add_trace(trace, row=1, col=1)\n",
    "\n",
    "for trace in traces_Sushiswap:\n",
    "    fig.add_trace(trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=1200, title_text=\"Three-month pool returns over analysis timeframe\")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Return (%)\", row=1, col=2)\n",
    "\n",
    "offline.init_notebook_mode(connected=True)\n",
    "offline.plot(fig, filename='combined_profit_plot.html', auto_open=False)\n",
    "offline.iplot(fig)\n",
    "pio.write_image(fig, '/Users/fabioza/Desktop/Master thesis/sushi_uni_3m_returns.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa730d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pools_df = complete_pools_df.reset_index()\n",
    "\n",
    "#Calculating daily returns, based on returns the 90d vola and impermanent loss for each pair in the liquidity pool\n",
    "complete_pools_df['date'] = pd.to_datetime(complete_pools_df['date'])\n",
    "complete_pools_df = complete_pools_df.sort_values(['pool', 'date'])\n",
    "complete_pools_df['daily_return'] = complete_pools_df.groupby('pool')['avg_swap_price'].pct_change()\n",
    "\n",
    "#Vola as 90d stdev of daily returns\n",
    "complete_pools_df['90D_volatility'] = complete_pools_df.groupby('pool')['daily_return'].rolling(window=90).std().reset_index(0, drop=True)\n",
    "\n",
    "#impermanent loss as the percentage change of the ratio between the two tokens in the pool btw. time t_1 and t_2 \n",
    "def impermanent_loss(pt1, pt2):\n",
    "    ratio = pt2 / pt1\n",
    "    return 100 * ((2 * np.sqrt(ratio)) / (1 + ratio) - 1)\n",
    "\n",
    "complete_pools_df['pt1'] = complete_pools_df.groupby('pool')['avg_swap_price'].shift(89)\n",
    "complete_pools_df['90D_impermanent_loss'] = complete_pools_df.apply(lambda x: impermanent_loss(x['pt1'], x['avg_swap_price']) if pd.notnull(x['pt1']) else np.nan, axis=1)\n",
    "\n",
    "complete_pools_df = complete_pools_df.drop(columns=['pt1'])\n",
    "complete_pools_df = complete_pools_df.drop(columns=['daily_return'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pools_df = complete_pools_df.set_index('date')\n",
    "\n",
    "#Resampling and aggregating features on a 3-month horizon\n",
    "df_resampled = complete_pools_df.groupby('pool').resample('3M').agg({\n",
    "    'volume_usd': 'sum',\n",
    "    'imbalance': 'mean',\n",
    "    'avg_gas': 'mean',\n",
    "    '90D_volatility': 'last',\n",
    "    '90D_impermanent_loss': 'last',\n",
    "    'pool_category': 'first',\n",
    "    'total_USD_value': 'first',\n",
    "    'Uniswap': 'first',\n",
    "    'Sushiswap': 'first'\n",
    "})\n",
    "\n",
    "df_resampled.reset_index(inplace=True)\n",
    "\n",
    "#Making sure that date captures the first day of the month\n",
    "df_resampled['date'] = df_resampled['date'] + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "df_resampled.rename(columns={'date': 'start_date'}, inplace=True)\n",
    "\n",
    "#Adding profits (the feature we want to predict)\n",
    "df_final = pd.merge(df_resampled, profit_df, on=['pool', 'start_date'])\n",
    "\n",
    "#As we want to predict the return in three months from now we need to introduce this as a feature\n",
    "df_final['return_in_3months'] = df_final.groupby('pool')['return'].shift(-1)\n",
    "\n",
    "#The last observation for each pool is empty as there is no return in three months, this observation has to be dropped\n",
    "df_final = df_final.dropna(subset=['return_in_3months'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45653677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For one observation total_USD_value contains inf, I fill up this observation with the average of the observation before it and after it. This shouldn't bias results\n",
    "pool_value = \"0xb4e16d0168e52d35cacd2c6185b44281ec28c9dc\"\n",
    "pool_rows = df_final[df_final[\"pool\"] == pool_value]\n",
    "\n",
    "#Calculating average of row before and after\n",
    "filled_values = pool_rows[\"total_USD_value\"].rolling(3, min_periods=1, center=True).mean()\n",
    "\n",
    "#Overwriting the inf value\n",
    "df_final.loc[df_final[\"pool\"] == pool_value, \"total_USD_value\"] = filled_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a549c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data by date and then by pool for tree based models\n",
    "df_final = df_final.sort_values(by=['start_date', 'pool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33688cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a separate dataframe for recurrent networks\n",
    "df_final_RNN = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns\n",
    "df_final = df_final.drop(columns=['symbol_1', 'symbol_2', 'platform', 'start_date', 'end_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping na values\n",
    "df_final = df_final.dropna()\n",
    "df_final_RNN = df_final_RNN.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dccf64",
   "metadata": {},
   "source": [
    "## Summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_df_analysis = profit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421501e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning pool categories to pool contract addresses\n",
    "profit_df_analysis['pool_category'] = profit_df_analysis.apply(assign_category, axis=1)\n",
    "\n",
    "#Grouping by pool category and counting positive and negative returns and calculating stdev\n",
    "results = profit_df_analysis.groupby('pool_category')['return'].agg(\n",
    "    positive_count = lambda x: (x > 0).sum(),\n",
    "    negative_count = lambda x: (x < 0).sum(),\n",
    "    std_dev = 'std'\n",
    ")\n",
    "\n",
    "#Calculating ratios per category\n",
    "results['positive_ratio'] = results['positive_count'] / (results['positive_count'] + results['negative_count'])\n",
    "results['negative_ratio'] = results['negative_count'] / (results['positive_count'] + results['negative_count'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number and proportion of pools with negative and positive returns on Uniswap\n",
    "pos_Uniswap = len(profit_df_Uniswap[profit_df_Uniswap[\"return\"] >= 0])\n",
    "neg_Uniswap = len(profit_df_Uniswap[profit_df_Uniswap[\"return\"] < 0])\n",
    "print(pos_Uniswap)\n",
    "print(neg_Uniswap)\n",
    "print(pos_Uniswap / (pos_Uniswap + neg_Uniswap))\n",
    "\n",
    "#Number and proportion of pools with negative and positive returns on Sushiswap\n",
    "pos_Sushiswap = len(profit_df_Sushiswap[profit_df_Sushiswap[\"return\"] >= 0])\n",
    "neg_Sushiswap = len(profit_df_Sushiswap[profit_df_Sushiswap[\"return\"] < 0])\n",
    "print(pos_Sushiswap)\n",
    "print(neg_Sushiswap)\n",
    "print(pos_Sushiswap / (pos_Sushiswap + neg_Sushiswap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5092cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive vs. negative returns before forecasting / testing period\n",
    "filtered_df = df_final_RNN[df_final_RNN[\"start_date\"] < \"2022-08-01\"]\n",
    "\n",
    "num_positives = filtered_df[filtered_df[\"return_in_3months\"] > 0].shape[0]\n",
    "num_negatives = filtered_df[filtered_df[\"return_in_3months\"] < 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive returns: {num_positives}\")\n",
    "print(f\"Number of negative returns: {num_negatives}\")\n",
    "print(f\"Ratio pos.: {num_positives / (num_positives+num_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f83606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive vs. negative returns during forecasting / testing period\n",
    "filtered_df = df_final_RNN[df_final_RNN[\"start_date\"] > \"2022-08-01\"]\n",
    "\n",
    "num_positives = filtered_df[filtered_df[\"return_in_3months\"] > 0].shape[0]\n",
    "num_negatives = filtered_df[filtered_df[\"return_in_3months\"] < 0].shape[0]\n",
    "\n",
    "print(f\"Number of positive returns: {num_positives}\")\n",
    "print(f\"Number of negative returns: {num_negatives}\")\n",
    "print(f\"Ratio neg. to pos.: {num_negatives}\")\n",
    "print(f\"Ratio pos.: {num_positives / (num_positives+num_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e51fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General descriptive stats\n",
    "numerical_cols = ['volume_usd', 'imbalance', 'avg_gas', '90D_volatility', '90D_impermanent_loss', 'total_USD_value', 'return', 'return_in_3months']\n",
    "\n",
    "df_final[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f25ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive stats per pool category\n",
    "df_final.groupby('pool_category')[numerical_cols].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c761346",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling and aggregating features on a 3-month horizon\n",
    "df_resampled = complete_pools_df.groupby('pool').resample('3M').agg({\n",
    "    'volume_usd': 'sum',\n",
    "    'imbalance': 'mean',\n",
    "    'avg_gas': 'mean',\n",
    "    '90D_volatility': 'last',\n",
    "    '90D_impermanent_loss': 'last',\n",
    "    'pool_category': 'first',\n",
    "    'total_USD_value': 'first',\n",
    "    'Uniswap': 'first',\n",
    "    'Sushiswap': 'first'\n",
    "})\n",
    "\n",
    "df_resampled.reset_index(inplace=True)\n",
    "\n",
    "#Making sure that date captures the first day of the month\n",
    "df_resampled['date'] = df_resampled['date'] + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "df_resampled.rename(columns={'date': 'start_date'}, inplace=True)\n",
    "\n",
    "#Adding profits (the feature we want to predict)\n",
    "df_final_linear = pd.merge(df_resampled, profit_df, on=['pool', 'start_date'])\n",
    "\n",
    "#As we want to predict the return in three months from now we need to introduce this as a feature\n",
    "df_final_linear['return_in_3months'] = df_final_linear.groupby('pool')['return'].shift(-1)\n",
    "\n",
    "#The last observation for each pool is empty as there is no return in three months, this observation has to be dropped\n",
    "df_final_linear = df_final_linear.dropna(subset=['return_in_3months'])\n",
    "\n",
    "#Dropping first row for each pool which now has NaN values (as there were no observations three months ago)\n",
    "df_final_linear.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preprocessing the data and getting it into the rigth format\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_final_linear[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final_linear[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Defining the features and the target\n",
    "df_final_linear = df_final_linear.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded_linear = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    df_final_linear.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded_linear['total_USD_value'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "col_index = df_encoded_linear.columns.get_loc('total_USD_value')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "total_usd_value = df_encoded_linear.iloc[:, col_index].values.reshape(-1, 1)\n",
    "df_encoded_linear.iloc[:, col_index] = imputer.fit_transform(total_usd_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "df_encoded_linear = df_encoded_linear.drop(columns = [\"symbol_1\", \"symbol_2\", \"end_date\", \"platform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting df to csv and saving locally\n",
    "df_encoded_linear.to_csv(\"/Users/fabioza/Desktop/Master thesis/data/df_encoded_linear.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code continued in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76ee56",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further preprocessing the data and getting it into the rigth format to avoid leakage due to time-series nature of data\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Defining the features and the target\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category', 'return_in_3months'])\n",
    "y = df_no_missing['return_in_3months']\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_encoded_filled = pd.DataFrame(imputer.fit_transform(df_encoded), columns=df_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "#According to this 4 3-month periods are used for training, 2 for validation and 2 for testing\n",
    "train_size = 0.5 \n",
    "val_size = 0.25\n",
    "test_size = 0.25\n",
    "\n",
    "#Calculate the sizes based on the proportions\n",
    "num_samples = len(df_encoded_filled)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded_filled[:train_samples]\n",
    "X_val = df_encoded_filled[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded_filled[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Initializing the random forest model\n",
    "model = RandomForestRegressor(random_state=63)\n",
    "\n",
    "#Defining parameter grid (number of trees and depth of each tree)\n",
    "param_grid = {\n",
    "    'n_estimators': (10, 1000),  \n",
    "    'max_depth': (1, 20),\n",
    "}\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series anture of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to six observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "#Expanding window is being used, which means that in the first iteration the first validation fold is being forecasted and\n",
    "#in the second run, it is being added to the training set and the last validation fold is being forecasted\n",
    "window_length = 2\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "        \n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890678f",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with Gradient Boosting with grid search and bayesian optimization\n",
    "#Best parameters with hyperparam search are found based on train / validation set\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#Defining target and features\n",
    "y = df_no_missing['return_in_3months']\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "X = df_encoded.drop(columns=['return_in_3months'])\n",
    "\n",
    "#Initializing the gradient boosting model\n",
    "model = GradientBoostingRegressor(random_state=63)\n",
    "\n",
    "#Defining parameter grid (number of trees, depth of each tree and learning rate)\n",
    "param_grid = {\n",
    "    'n_estimators': (50, 1000),   \n",
    "    'max_depth': (2, 20),          \n",
    "    'learning_rate': (0.001, 1)   \n",
    "}\n",
    "\n",
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "#According to this 4 3-month periods are used for training, 2 for validation and 2 for testing\n",
    "train_size = 0.5 \n",
    "val_size = 0.25\n",
    "test_size = 0.25\n",
    "\n",
    "#Calculating the sizes based on the proportions\n",
    "num_samples = len(df_encoded)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded[:train_samples]\n",
    "X_val = df_encoded[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series anture of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to six observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "#Expanding window is being used, which means that in the first iteration the first validation fold is being forecasted and\n",
    "#in the second run, it is being added to the training set and the last validation fold is being forecasted\n",
    "window_length = 2\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    \n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000d379",
   "metadata": {},
   "source": [
    "## Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One with Bayesian Optimization, stochastic gradient descent and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_3months']))\n",
    "y = df_encoded['return_in_3months']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        #DNN has one hidden layer, a batch normalization layer, an output layer and a dropout layer\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X.shape[1], 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64) \n",
    "        self.fc2 = nn.Linear(64, 1)  \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    #Different optimizers such as adam or adagrad have been tested out. SGD performs the best\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        #Splitting up the data into training and validation features + labels\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        #Converting training and validation data to tensor\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))\n",
    "    \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96b923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Three with Bayesian Optimization (SGD) and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_3months']))\n",
    "y = df_encoded['return_in_3months']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        #DNN has three hidden layers, three batch normalization layer, an output layer and a dropout layer\n",
    "        self.fc1 = nn.Linear(X.shape[1], 256)  \n",
    "        self.bn1 = nn.BatchNorm1d(256) \n",
    "        self.fc2 = nn.Linear(256, 128) \n",
    "        self.bn2 = nn.BatchNorm1d(128)  \n",
    "        self.fc3 = nn.Linear(128, 64)   \n",
    "        self.bn3 = nn.BatchNorm1d(64)   \n",
    "        self.fc4 = nn.Linear(64, 1)     \n",
    "        self.dropout = nn.Dropout(dropout_rate)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))   \n",
    "        \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five with Bayesian Optimization (SGD) and time-series cross-validation\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_final[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_final.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_final[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool', 'pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "#Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_encoded.drop(columns=['return_in_3months']))\n",
    "y = df_encoded['return_in_3months']\n",
    "\n",
    "#Defining a train and validation split. The test data can later be used to test the best model\n",
    "#Split is same as before\n",
    "train_size = int(0.75 * len(X))  \n",
    "test_size = len(X) - train_size\n",
    "\n",
    "#Preparing train and test data\n",
    "train_X, train_y = X[:train_size], y[:train_size]\n",
    "test_X, test_y = X[train_size:], y[train_size:]\n",
    "\n",
    "#Converting test features and target to pytorch tensors (might have to use them later)\n",
    "test_inputs = torch.tensor(test_X, dtype=torch.float32).view(-1, X.shape[1])\n",
    "test_labels = torch.tensor(test_y.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Defining the architecture of the deep neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        #DNN has five hidden layers, five batch normalization layer, an output layer and a dropout layer\n",
    "        self.fc1 = nn.Linear(X.shape[1], 256)  \n",
    "        self.bn1 = nn.BatchNorm1d(256)  \n",
    "        self.fc2 = nn.Linear(256, 128)  \n",
    "        self.bn2 = nn.BatchNorm1d(128)  \n",
    "        self.fc3 = nn.Linear(128, 64)  \n",
    "        self.bn3 = nn.BatchNorm1d(64)  \n",
    "        self.fc4 = nn.Linear(64, 32)  \n",
    "        self.bn4 = nn.BatchNorm1d(32)  \n",
    "        self.fc5 = nn.Linear(32, 16)  \n",
    "        self.bn5 = nn.BatchNorm1d(16)  \n",
    "        self.fc6 = nn.Linear(16, 1) \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #activation function is relu, others have been tried out but didn't perform better\n",
    "        #defining how the layers are arranged\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "#setting the num of folds for time-series cross validation\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def cross_val_score(lr: float, weight_decay: float, dropout_rate: float):\n",
    "    batch_size = 30\n",
    "    model = Net(dropout_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    \n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "    hit_rates = []\n",
    "    \n",
    "    #Iterating over each time-series split\n",
    "    for train_index, val_index in tscv.split(train_X):\n",
    "        train_inputs, train_labels = torch.tensor(train_X[train_index], dtype=torch.float32), torch.tensor(train_y.iloc[train_index].values, dtype=torch.float32).view(-1, 1)\n",
    "        val_inputs, val_labels = torch.tensor(train_X[val_index], dtype=torch.float32), torch.tensor(train_y.iloc[val_index].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataset = TensorDataset(val_inputs, val_labels)\n",
    "        valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        #Training the model\n",
    "        for epoch in range(250):\n",
    "            model.train()\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #Validating the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                all_preds, all_labels = [], []\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    all_preds.extend(output.view(-1).tolist())\n",
    "                    all_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(all_labels, all_preds)))\n",
    "        mae_scores.append(mean_absolute_error(all_labels, all_preds)) \n",
    "        hit_rates.append(np.mean((np.array(all_preds) > 0) == (np.array(all_labels) > 0)))   \n",
    "        \n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "    \n",
    "    return -np.mean(mae_scores)  \n",
    "\n",
    "#Defining parameter grid\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Optimizing the hyperparameter search with bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=lambda lr, weight_decay, dropout_rate: cross_val_score(lr, weight_decay, dropout_rate),\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e6f13",
   "metadata": {},
   "source": [
    "## GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c5344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for recurrent neural networks\n",
    "df_no_missing = df_final_RNN.dropna()\n",
    "\n",
    "#Encoding \"pool\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([pd.DataFrame(pool_encoded, columns=pool_columns), df_no_missing.reset_index(drop=True)], axis=1)\n",
    "\n",
    "#Encoding \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([df_encoded, pd.DataFrame(pool_category_encoded, columns=pool_category_columns)], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(columns = ['pool_category'])\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded['total_USD_value'].fillna(df_encoded['total_USD_value'].mean(), inplace=True)\n",
    "\n",
    "#Dropping unnecessary columns\n",
    "df_encoded = df_encoded.drop(columns = [\"platform\", \"end_date\", \"symbol_1\", \"symbol_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6989f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trying out a GRU model\n",
    "#Code to create sequences from the input data\n",
    "def create_sequences(input_data, labels, seq_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "\n",
    "    for i in range(len(input_data) - seq_length + 1):\n",
    "        sequence = input_data[i:i+seq_length].values  \n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)  \n",
    "        sequences.append(sequence)\n",
    "        seq_labels.append(labels[i+seq_length-1])\n",
    "\n",
    "    return torch.stack(sequences), torch.tensor(seq_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#Splitting data into train and test sets (75% of data is used for training, 25% for testing)\n",
    "train_size = int(0.75 * len(df_encoded))\n",
    "test_size = len(df_encoded) - train_size\n",
    "\n",
    "train_data = df_encoded[:train_size]\n",
    "test_data = df_encoded[train_size:]\n",
    "\n",
    "train_data = train_data.sort_values(by=['pool', 'start_date'])\n",
    "test_data = test_data.sort_values(by=['pool', 'start_date'])\n",
    "\n",
    "train_data = train_data.drop(columns = ['pool', 'start_date'])\n",
    "test_data = test_data.drop(columns = ['pool', 'start_date'])\n",
    "\n",
    "#The longer the more historic info taken into account but also the more complex training\n",
    "seq_length = 2\n",
    "\n",
    "#Bring input data for each pool into the rigth format (there's 35 pools with 8 observations each)\n",
    "train_sequences = []\n",
    "train_labels = []\n",
    "for i in range(35):\n",
    "    pool_df = train_data[i*6:(i+1)*6].reset_index(drop=True)\n",
    "    pool_inputs, pool_labels = create_sequences(pool_df.drop(columns=['return_in_3months']), \n",
    "                                                pool_df['return_in_3months'], seq_length)\n",
    "    train_sequences.append(pool_inputs)\n",
    "    train_labels.append(pool_labels)\n",
    "    \n",
    "seq_length = 1\n",
    "\n",
    "#Same for test data\n",
    "test_sequences = []\n",
    "test_labels = []\n",
    "for i in range(35):\n",
    "    pool_df = test_data[i*2:(i+1)*2].reset_index(drop=True)\n",
    "    pool_inputs, pool_labels = create_sequences(pool_df.drop(columns=['return_in_3months']), \n",
    "                                                pool_df['return_in_3months'], seq_length)\n",
    "    test_sequences.append(pool_inputs)\n",
    "    test_labels.append(pool_labels)\n",
    "\n",
    "#Converting train_sequences and test_sequences to lists of tensors\n",
    "train_sequences = torch.cat(train_sequences)\n",
    "train_labels = torch.cat(train_labels)\n",
    "test_sequences = torch.cat(test_sequences)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n",
    "#Converting training and validation data to tensor form\n",
    "train_data = TensorDataset(train_sequences, train_labels)\n",
    "test_data = TensorDataset(test_sequences, test_labels)\n",
    "\n",
    "#Provides model with batches of training and validation data \n",
    "#No shuffling to preserve time series nature of data\n",
    "trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "testloader = DataLoader(test_data, batch_size=30, shuffle=False)\n",
    "\n",
    "#Defining model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "#Setting up time-series cross val\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "def train_model(lr: float, weight_decay: float, hidden_dim: int, n_layers: int, dropout_rate: float):\n",
    "    #To return the best model\n",
    "    global best_model\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    n_layers = int(n_layers)\n",
    "    model = Net(input_dim=train_sequences.shape[2], hidden_dim=hidden_dim, n_layers=n_layers, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "    best_score = np.inf\n",
    "    \n",
    "    best_model = None\n",
    "\n",
    "    mse_scores, rmse_scores, mae_scores, hit_rates = [], [], [], []\n",
    "\n",
    "    for train_index, val_index in tscv.split(train_sequences):\n",
    "        train_split = train_sequences[train_index]\n",
    "        val_split = train_sequences[val_index]\n",
    "\n",
    "        train_split_labels = train_labels[train_index]\n",
    "        val_split_labels = train_labels[val_index]\n",
    "\n",
    "        train_data = TensorDataset(train_split, train_split_labels)\n",
    "        val_data = TensorDataset(val_split, val_split_labels)\n",
    "\n",
    "        trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "        valloader = DataLoader(val_data, batch_size=30, shuffle=False)\n",
    "\n",
    "        model.train()\n",
    "        epochs = 1000\n",
    "        best_loss = np.inf\n",
    "        patience = 20\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in trainloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valloader:\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "                    mse = mean_squared_error(labels.detach().numpy(), output.detach().numpy())\n",
    "                    rmse = np.sqrt(mse)\n",
    "                    mae = mean_absolute_error(labels.detach().numpy(), output.detach().numpy())\n",
    "                    hit_rate = np.mean((np.sign(labels.detach().numpy()) == np.sign(output.detach().numpy())))\n",
    "\n",
    "                    mse_scores.append(mse)\n",
    "                    rmse_scores.append(rmse)\n",
    "                    mae_scores.append(mae)\n",
    "                    hit_rates.append(hit_rate)\n",
    "\n",
    "            scheduler.step(val_running_loss)\n",
    "\n",
    "            if val_running_loss < best_loss:\n",
    "                best_loss = val_running_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "        if best_loss < best_score:\n",
    "            best_score = best_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    print(f'Mean MSE: {np.mean(mse_scores)}')\n",
    "    print(f'Mean RMSE: {np.mean(rmse_scores)}')\n",
    "    print(f'Mean MAE: {np.mean(mae_scores)}')\n",
    "    print(f'Mean Hit rate: {np.mean(hit_rates)}')\n",
    "\n",
    "    return -best_score\n",
    "\n",
    "#Defining grid for hyperparam search with bayesian opt\n",
    "bounds = {\n",
    "    'lr': (0.00001, 0.01),\n",
    "    'weight_decay': (0.000001, 0.001),\n",
    "    'hidden_dim': (50, 500),\n",
    "    'n_layers': (1, 3),\n",
    "    'dropout_rate': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "#Defining bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=63,\n",
    ")\n",
    "\n",
    "#Optimizer runs 25 times, changing three params each\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter= 1\n",
    ")\n",
    "\n",
    "print(optimizer.max)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c82b50",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out a LSTM model\n",
    "#Code to create sequences from the input data\n",
    "def create_sequences(input_data, labels, seq_length):\n",
    "    sequences = []\n",
    "    seq_labels = []\n",
    "\n",
    "    for i in range(len(input_data) - seq_length + 1):\n",
    "        sequences.append(input_data[i:i+seq_length])  \n",
    "        seq_labels.append(labels[i+seq_length-1])  \n",
    "\n",
    "    return torch.stack(sequences), torch.tensor(seq_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#The longer the more historic info taken into account but also the more complex training\n",
    "seq_length = 6\n",
    "\n",
    "#Bring input data for each pool into the rigth format (there's 35 pools with 8 observations each)\n",
    "pool_data = []\n",
    "for i in range(35):\n",
    "    #Pool_df contains observations on each pool\n",
    "    pool_df = df_encoded[i*8:(i+1)*8]\n",
    "    #Extracting features\n",
    "    pool_inputs = torch.tensor(pool_df.drop(columns=['return_in_3months']).values, dtype=torch.float32)\n",
    "    #Extracting target\n",
    "    pool_labels = torch.tensor(pool_df['return_in_3months'].values, dtype=torch.float32)\n",
    "    #Creating the sequences of features and target\n",
    "    pool_sequences, pool_seq_labels = create_sequences(pool_inputs, pool_labels, seq_length)\n",
    "    pool_data.append((pool_sequences, pool_seq_labels))\n",
    "\n",
    "#Concat data from all pools\n",
    "inputs = torch.cat([data[0] for data in pool_data])\n",
    "labels = torch.cat([data[1] for data in pool_data])\n",
    "\n",
    "#Split data into train and validation sets (75% of data is used for training, 25% for validation)\n",
    "#We would also need to define a test set but performance is very far off from other models, which means we wouldn't use this model anywaystrain_size = int(0.75 * len(inputs))\n",
    "train_sequences, train_labels = inputs[:train_size], labels[:train_size]\n",
    "val_sequences, val_labels = inputs[train_size:], labels[train_size:]\n",
    "\n",
    "#Converting training and validation data to tensor form\n",
    "train_data = TensorDataset(train_sequences, train_labels)\n",
    "val_data = TensorDataset(val_sequences, val_labels)\n",
    "\n",
    "#Provides model with batches of training and validation data \n",
    "#No shuffling to preserve time series nature of data\n",
    "trainloader = DataLoader(train_data, batch_size=30, shuffle=False)\n",
    "valloader = DataLoader(val_data, batch_size=30, shuffle=False)\n",
    "\n",
    "#Defining model architecture\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout_rate):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def train_model(lr, weight_decay, hidden_dim, n_layers, dropout_rate):\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    n_layers = int(n_layers)\n",
    "    model = LSTMNet(input_dim=train_sequences.shape[2], hidden_dim=hidden_dim, n_layers=n_layers, dropout_rate=dropout_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n",
    "\n",
    "    #Training the model\n",
    "    epochs = 1000\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valloader:\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        scheduler.step(val_running_loss)\n",
    "\n",
    "        #Early stopping if no further progress is being made in training\n",
    "        if val_running_loss < best_loss:\n",
    "            best_loss = val_running_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "    #negative loss is metric we want to maximize (want to minimize loss)\n",
    "    return -best_loss\n",
    "\n",
    "#Defining grid for hyperparam search with bayesian opt\n",
    "bounds = {\n",
    "    'lr': (1e-5, 1e-2),\n",
    "    'weight_decay': (1e-6, 1e-3),\n",
    "    'hidden_dim': (32, 128),\n",
    "    'n_layers': (1, 5),\n",
    "    'dropout_rate': (0.0, 0.7)\n",
    "}\n",
    "\n",
    "#Defining bayesian optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "#Optimizer runs 50 times, changing five params each\n",
    "optimizer.maximize(init_points=5, n_iter=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e07f7d2",
   "metadata": {},
   "source": [
    "## Re-training the best model and testing it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fac69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model with Gradient Boosting with grid search and bayesian optimization\n",
    "#Best parameters with hyperparam search are found based on train / validation set and performance of best model is then evaluated on test set\n",
    "#This is being done as gradient boosting has best performance on training / validation and is therefore best model overall\n",
    "df_no_missing = df_final.dropna()\n",
    "\n",
    "#Defining target and features\n",
    "y = df_no_missing['return_in_3months']\n",
    "X = df_no_missing.drop(columns=['pool', 'pool_category'])\n",
    "\n",
    "#Encoding \"pool\" and \"pool_category\" as one-hot (adding new columns and adding 1 or 0 in the corresponding rows)\n",
    "encoder_pool = OneHotEncoder(sparse_output=False)\n",
    "pool_encoded = encoder_pool.fit_transform(df_no_missing[['pool']])\n",
    "pool_columns = ['pool_' + str(i) for i in range(pool_encoded.shape[1])]\n",
    "\n",
    "encoder_pool_category = OneHotEncoder(sparse_output=False)\n",
    "pool_category_encoded = encoder_pool_category.fit_transform(df_no_missing[['pool_category']])\n",
    "pool_category_columns = ['pool_category_' + str(i) for i in range(pool_category_encoded.shape[1])]\n",
    "\n",
    "#Concatenating one hot-encoded and other features to same dataframe\n",
    "df_encoded = pd.concat([\n",
    "    pd.DataFrame(pool_encoded, columns=pool_columns),\n",
    "    pd.DataFrame(pool_category_encoded, columns=pool_category_columns),\n",
    "    X.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "#One value in the pool size columns couldn't be calculated, it's replaced with the mean which shouldn't bias results\n",
    "df_encoded.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_encoded.fillna(df_encoded.mean(), inplace=True)\n",
    "\n",
    "X = df_encoded.drop(columns=['return_in_3months'])\n",
    "\n",
    "#Initializing the gradient boosting model, adding regularization and early stopping to avoid overfitting the model\n",
    "model = GradientBoostingRegressor(min_samples_split = 10, min_samples_leaf = 4, \n",
    "                                  random_state=63, max_depth=3, max_features='sqrt', \n",
    "                                  subsample=0.8, n_iter_no_change=5, tol=0.01)\n",
    "\n",
    "#Defining parameter grid (number of trees, depth of each tree and learning rate)\n",
    "param_grid = {\n",
    "    'n_estimators': (1, 100),   \n",
    "    'max_depth': (1, 5),          \n",
    "    'learning_rate': (0.001, 0.1)   \n",
    "}\n",
    "\n",
    "#Splitting data into train / validating and test set\n",
    "#Train and validation sets are used to find the best model, test set can later be used to test the best model\n",
    "#According to this 4 3-month periods are used for training, 2 for validation and 2 for testing\n",
    "train_size = 0.5\n",
    "val_size = 0.25\n",
    "test_size = 0.25\n",
    "\n",
    "#Calculating the sizes based on the proportions\n",
    "num_samples = len(df_encoded)\n",
    "train_samples = int(train_size * num_samples)\n",
    "val_samples = int(val_size * num_samples)\n",
    "test_samples = num_samples - train_samples - val_samples\n",
    "\n",
    "X_train = df_encoded[:train_samples]\n",
    "X_val = df_encoded[train_samples:train_samples + val_samples]\n",
    "X_test = df_encoded[train_samples + val_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_val = y[train_samples:train_samples + val_samples]\n",
    "y_test = y[train_samples + val_samples:]\n",
    "\n",
    "#Concatenating train and validation dfs\n",
    "X_trainval = pd.concat([X_train, X_val])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "#Performing time-series cross-validation to account for time-series nature of data\n",
    "#75% of the total data is in the train / validation set, this corresponds to six observations per pool (which explains the split)\n",
    "#Each fold is being used once as the validation set and the rest as the training set\n",
    "tscv = TimeSeriesSplit(n_splits=6)\n",
    "\n",
    "#Using bayes optimization to find the best hyperparam combination for my model based on the grid defined earlier\n",
    "bayes_search = BayesSearchCV(model, param_grid, scoring='neg_mean_absolute_error', cv=tscv, n_jobs=-1, n_iter=50)\n",
    "bayes_search.fit(X_trainval, y_trainval)\n",
    "\n",
    "#Finding the best model (maximizing negative MAE (=minimizing MAE))\n",
    "best_model = bayes_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score (Negative MAE):\", bayes_search.best_score_)\n",
    "\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "hit_rate_list = []\n",
    "\n",
    "#Expanding window is being used, which means that in the first iteration the first validation fold is being forecasted and\n",
    "#in the second run, it is being added to the training set and the last validation fold is being forecasted\n",
    "window_length = 2\n",
    "\n",
    "fold_num = 1\n",
    "\n",
    "#Fitting the previously found best model to the training data and evaluating it on the validation data\n",
    "for train_index, val_index in tscv.split(X_trainval):\n",
    "    X_train, X_val = X_trainval.iloc[train_index], X_trainval.iloc[val_index]\n",
    "    y_train, y_val = y_trainval.iloc[train_index], y_trainval.iloc[val_index]\n",
    "    \n",
    "    if len(X_train) >= window_length:\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "        #Calculating evaluation metrics on the validation set\n",
    "        mse_val = mean_squared_error(y_val, y_pred_val)\n",
    "        mse_list.append(mse_val)\n",
    "        rmse_val = math.sqrt(mse_val)\n",
    "        rmse_list.append(rmse_val)\n",
    "        mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "        mae_list.append(mae_val)\n",
    "        hit_rate_val = np.mean((y_val > 0) == (y_pred_val > 0))\n",
    "        hit_rate_list.append(hit_rate_val)\n",
    "        fold_num += 1\n",
    "\n",
    "print(f\"Average MSE on Validation Set: {np.mean(mse_list)}\")\n",
    "print(f\"Average RMSE on Validation Set: {np.mean(rmse_list)}\")\n",
    "print(f\"Average MAE on Validation Set: {np.mean(mae_list)}\")\n",
    "print(f\"Average Hit Rate on Validation Set: {np.mean(hit_rate_list)}\")\n",
    "\n",
    "plt.plot(range(1, fold_num), mae_list, marker='o')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Mean Absolute Error for Each Fold')\n",
    "plt.show()\n",
    "\n",
    "actual_values = []\n",
    "predicted_values = []\n",
    "lower_values = []\n",
    "upper_values = [] \n",
    "\n",
    "#Splitting data into train_val and test sets\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=test_samples, shuffle=False)\n",
    "\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)\n",
    "\n",
    "X_test_copy = X_test.copy()\n",
    "\n",
    "#Adding a period column to the test set to apply a expanding window approach\n",
    "for i in range(pool_encoded.shape[1]):\n",
    "    X_test_copy['pool_'+str(i)] = X_test_copy['pool_'+str(i)].cumsum()\n",
    "\n",
    "X_test_copy['period'] = X_test_copy[['pool_' + str(i) for i in range(pool_encoded.shape[1])]].max(axis=1)\n",
    "\n",
    "#Training two additional models to predict the 5th and 95th percentiles (to predict uncertainty)\n",
    "model_lower = GradientBoostingRegressor(loss='quantile', alpha=0.05, **bayes_search.best_params_)\n",
    "model_upper = GradientBoostingRegressor(loss='quantile', alpha=0.95, **bayes_search.best_params_)\n",
    "\n",
    "#Fitting best model on train_val set\n",
    "best_model.fit(X_trainval, y_trainval)\n",
    "model_lower.fit(X_trainval, y_trainval)\n",
    "model_upper.fit(X_trainval, y_trainval)\n",
    "\n",
    "periods = X_test_copy['period'].unique()\n",
    "\n",
    "#Applying walk forward approach (first predicting the first test period, then adding it to training set, refitting the model\n",
    "#and predicting the second period)\n",
    "for i in range(2):\n",
    "    X_test_copy_period = X_test_copy[X_test_copy['period'] == periods[i]]\n",
    "    X_test_period = X_test.loc[X_test_copy_period.index]\n",
    "    y_test_period = y_test.loc[X_test_copy_period.index]\n",
    "    y_test_period = y_test_period.reset_index(drop = True)\n",
    "    X_test_copy_period = X_test_copy_period.drop('period', axis=1)\n",
    "\n",
    "    #Predicting target variable in test set\n",
    "    y_pred_test = best_model.predict(X_test_period)\n",
    "    y_pred_lower = model_lower.predict(X_test_period)\n",
    "    y_pred_upper = model_upper.predict(X_test_period)\n",
    "    \n",
    "    #Storing actual values, predicted values and the upper and lower bounds of the forecast\n",
    "    actual_values.extend(y_test_period)\n",
    "    predicted_values.extend(y_pred_test)\n",
    "    lower_values.extend(y_pred_lower)  \n",
    "    upper_values.extend(y_pred_upper)\n",
    "\n",
    "    #Calculating eval metrics on test set\n",
    "    mse_test = mean_squared_error(y_test_period, y_pred_test)\n",
    "    rmse_test = math.sqrt(mse_test)\n",
    "    mae_test = mean_absolute_error(y_test_period, y_pred_test)\n",
    "    hit_rate_test = np.mean((y_test_period > 0) == (y_pred_test > 0))\n",
    "\n",
    "    print(f\"Period {periods[i]}:\")\n",
    "    print(f\"MSE on Test Set: {mse_test}\")\n",
    "    print(f\"RMSE on Test Set: {rmse_test}\")\n",
    "    print(f\"MAE on Test Set: {mae_test}\")\n",
    "    print(f\"Hit Rate on Test Set: {hit_rate_test}\")\n",
    "\n",
    "    #Adding data of the current testing period to the trainval set\n",
    "    X_trainval = pd.concat([X_trainval, X_test_period])\n",
    "    y_trainval = pd.concat([y_trainval, y_test_period])\n",
    "\n",
    "   #Refitting model on new training set\n",
    "    best_model.fit(X_trainval, y_trainval)\n",
    "    model_lower.fit(X_trainval, y_trainval)\n",
    "    model_upper.fit(X_trainval, y_trainval)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importances from best model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "#Get corresponding feature names\n",
    "feature_names = X_trainval.columns\n",
    "\n",
    "#Select top 10 features\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "top_indices = sorted_indices[:10]\n",
    "top_importances = sorted_importances[:10]\n",
    "top_feature_names = sorted_feature_names[:10]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(top_importances)), top_importances, tick_label=top_feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715eac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plotting 95% confidence interval around predictions\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "#Lineplot for actual and predicted values\n",
    "ax.plot(range(len(actual_values)), actual_values, color='blue', label='Actual Return')\n",
    "ax.plot(range(len(predicted_values)), predicted_values, color='red', label='Predicted Return')\n",
    "\n",
    "#Plotting the confidence interval around predictions\n",
    "ax.fill_between(range(len(predicted_values)), lower_values, upper_values, color='grey', alpha=.5, label='Confidence Interval')\n",
    "\n",
    "ax.set_title(\"Predicted vs Actual Returns with a 90% Confidence Interval\")\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Return (%)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the addresses of the pools\n",
    "pool_names = df_final[\"pool\"].unique()\n",
    "\n",
    "#Doubling the list\n",
    "pool_names = np.concatenate((pool_names, pool_names))\n",
    "\n",
    "#Creating a period list (first 35 observations are in the the first period, ...)\n",
    "periods_list = [1 if i < 35 else 2 for i in range(70)]\n",
    "\n",
    "#Adding all the relevant lists to a dataframe\n",
    "df_results = pd.DataFrame({\n",
    "    'Pool': pool_names,\n",
    "    'Period': periods_list,\n",
    "    'Actual_Return': actual_values,\n",
    "    'Predicted_Return': predicted_values\n",
    "})\n",
    "\n",
    "#Grouping by periods and finding the pool with the highest predicted return\n",
    "for period, group in df_results.groupby('Period'):\n",
    "    max_pred_return_pool = group.loc[group['Predicted_Return'].idxmax(), 'Pool']\n",
    "    max_pred_return = group.loc[group['Predicted_Return'].idxmax(), 'Predicted_Return']\n",
    "    actual_return = group.loc[group['Predicted_Return'].idxmax(), 'Actual_Return']\n",
    "    \n",
    "    print(f\"Period {period}:\")\n",
    "    print(f\"Pool with highest predicted return: {max_pred_return_pool}\")\n",
    "    print(f\"Predicted Return: {max_pred_return}\")\n",
    "    print(f\"Actual Return: {actual_return}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e3ec1c",
   "metadata": {},
   "source": [
    "## Plot Uniswap vs. Sushiswap market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b97ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "univssushi = Sushiswap_pools_df = pd.read_csv(\"/Users/fabioza/Desktop/Master thesis/data/exchange comparison data/uni_vs_sushi.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eca707",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Converting dates to datetime\n",
    "univssushi['month'] = pd.to_datetime(univssushi['month'])\n",
    "\n",
    "#Pivoting dataframe\n",
    "df_pivot = univssushi.pivot(index='month', columns='project', values='usd_volume')\n",
    "\n",
    "#Subsetting data\n",
    "df_pivot = df_pivot[df_pivot.index >= '2020-09 -01']\n",
    "\n",
    "#Converting trading volumes to market share\n",
    "df_pivot = df_pivot.divide(df_pivot.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.stackplot(df_pivot.index, df_pivot.T, labels=df_pivot.columns, alpha=0.7)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))\n",
    "plt.title('Monthly Market Share of Uniswap vs Sushiswap vs Others')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Market Share')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
